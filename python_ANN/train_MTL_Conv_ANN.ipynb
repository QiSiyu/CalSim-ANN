{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQCaImHlxc54"
   },
   "source": [
    "## Train an MTL ANN (with trainable convolutional pre-processing layer) from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jkXdH1XQM3j8"
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------#\n",
    "#------------------------- User Settings --------------------------#\n",
    "#------------------------------------------------------------------#\n",
    "\n",
    "#### 1. Select one or more input variables from:\n",
    "# 'SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC'\n",
    "\n",
    "input_var = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
    "\n",
    "#### 2. Select one or more output stations from:\n",
    "# 'Emmaton', 'Jersey Point', 'Collinsville', 'Rock Slough', 'Antioch', 'Mallard',\n",
    "# 'LosVaqueros', 'Martinez', 'MiddleRiver', 'Vict Intake', 'CVP Intake', 'CCFB_OldR'\n",
    "\n",
    "output_stations=['Emmaton', 'Jersey Point', 'Collinsville', 'Rock Slough', 'Antioch', 'Mallard',\n",
    "                 'LosVaqueros', 'Martinez', 'MiddleRiver', 'Vict Intake', 'CVP Intake', 'CCFB_OldR']\n",
    "\n",
    "#### 3. Specify directory to excel dataset and the helper script (folder name only)\n",
    "google_drive_dir = 'python_ANN'\n",
    "\n",
    "#### 4. Specify number of nodes in hidden layers\n",
    "hidden_layer_nodes = [8*12, 2*12]\n",
    "\n",
    "#### 5. define lead days (days forecasting ahead) for each station\n",
    "lead_day = 0\n",
    "lead_day_for_station = {'Emmaton':lead_day,\n",
    "                        'Jersey Point':lead_day,\n",
    "                        'Collinsville':lead_day,\n",
    "                        'Rock Slough':lead_day,\n",
    "                        'Antioch':lead_day,\n",
    "                        'Mallard':lead_day,\n",
    "                        'LosVaqueros':lead_day,\n",
    "                        'Martinez':lead_day,\n",
    "                        'MiddleRiver':lead_day,\n",
    "                        'Vict Intake':lead_day,\n",
    "                        'CVP Intake':lead_day,\n",
    "                        'CCFB_OldR':lead_day}\n",
    "\n",
    "\n",
    "# 6. Specify whether:\n",
    "#     -- to train the ANN(s) ==> set to 'no'\n",
    "#     -- to quickly test the code, in which case ANN won't be well-trained ==> set to 'yes'\n",
    "is_quick_test = 'yes'\n",
    "\n",
    "# 7. Specify whether running the code on Colab or a local computer\n",
    "# NOTE: if set to False, please set option #3: google_drive_dir to the local path of python_ANN\n",
    "#       e.g., google_drive_dir='/Users/siyuqi/Downloads/Calsim-ANN-master/python_ANN'\n",
    "running_on_colab = True\n",
    "\n",
    "#------------------------------------------------------------------#\n",
    "#------------------- User Settings Finished -----------------------#\n",
    "#------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "!export PYTHONPATH=\"\"\n",
    "\n",
    "# Mount Google drive\n",
    "if running_on_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive',force_remount=True)\n",
    "    input_data_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"input_2.csv\")\n",
    "    output_data_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"output_2.csv\")\n",
    "    %tensorflow_version 1.x\n",
    "    sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir))\n",
    "else:\n",
    "    input_data_path = os.path.join(google_drive_dir,\"input_2.csv\")\n",
    "    output_data_path = os.path.join(google_drive_dir,\"output_2.csv\")\n",
    "    sys.path.append(os.path.join(google_drive_dir))\n",
    "\n",
    "if  'y' in is_quick_test.lower():\n",
    "    test_mode = True\n",
    "    print('Running for a quick test...')\n",
    "else:\n",
    "    test_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUaY9du2wmWw"
   },
   "source": [
    "Make sure user settings are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCzCj64kwmLl"
   },
   "outputs": [],
   "source": [
    "full_input_variable_list = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
    "full_output_station_list = ['Emmaton', 'Jersey Point', 'Collinsville', 'Rock Slough', 'Antioch', 'Mallard',\n",
    "                            'LosVaqueros', 'Martinez', 'MiddleRiver', 'Vict Intake', 'CVP Intake', 'CCFB_OldR']\n",
    "\n",
    "# check if input variables are valid\n",
    "assert len(set(input_var))==len(input_var), ('Duplicate input variable(s): ' + \", \".join(str(e) for e in set([x for x in input_var if input_var.count(x) > 1])) )\n",
    "assert len(list(set(input_var).intersection(full_input_variable_list)))==len(input_var), ('Invalid input variable(s): ' + \", \".join(str(e) for e in set([x for x in input_var if full_input_variable_list.count(x) == 0])) )\n",
    "\n",
    "# check if output stations are valid\n",
    "assert len(set(output_stations))==len(output_stations), ('Duplicate output station(s): ' + \", \".join(str(e) for e in set([x for x in output_stations if output_stations.count(x) > 1])) )\n",
    "assert len(list(set(output_stations).intersection(full_output_station_list)))==len(output_stations), ('Invalid output station(s): ' + \", \".join(str(e) for e in set([x for x in output_stations if full_output_station_list.count(x) == 0])) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0hVyNufqhc0"
   },
   "source": [
    "# Import useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK7YMXhnNMXa"
   },
   "outputs": [],
   "source": [
    "from ann_helper import read_csv,normalize_in,process_data,process_data_vary_pred,initnw,conv_filter_generator\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "# detect available device: CPU or GPU\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if 'gpu' not in device_name.lower():\n",
    "    print('Found CPU only')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VQKSR19qeH8"
   },
   "source": [
    "# Read and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl_fOymBNIrJ"
   },
   "outputs": [],
   "source": [
    "# define hyper-parameters\n",
    "single_daily = 8\n",
    "window_num = 10\n",
    "window_width = 11\n",
    "input_shape = (1,(single_daily+window_num)*len(input_var))\n",
    "output_shape = len(output_stations)\n",
    "\n",
    "nn_shape = [single_daily+window_num*window_width,]+hidden_layer_nodes+[output_shape,]\n",
    "\n",
    "\n",
    "# label and sort output stations by a given order\n",
    "locs = {'Emmaton':0,'Jersey Point':1,'Collinsville':2,'Rock Slough':3,'Antioch':4,\n",
    "        'Mallard':5, 'LosVaqueros':6, 'Martinez':7, 'MiddleRiver':8, 'Vict Intake':9,\n",
    "        'CVP Intake':10, 'CCFB_OldR':11}\n",
    "abbrev_map = {'rock slough':'ORRSL','rockslough':'ORRSL',\n",
    "            'emmaton':'EMM','jersey point':'JP','jerseypoint':'JP',\n",
    "            'antioch':'antioch','collinsville':'CO',\n",
    "            'mallard':'Mallard','mallard island':'Mallard',\n",
    "            'los vaqueros':'LosVaqueros','losvaqueros':'LosVaqueros',\n",
    "            'martinez':'MTZ',\n",
    "            'middle river':'MidR_intake','middleriver':'MidR_intake',\n",
    "            'victoria cannal':'Victoria_intake','vict intake':'Victoria_intake',\n",
    "            'cvp intake':'CVP_intake','clfct forebay':'CCFB',\n",
    "            'clfct forebay intake':'ccfb_intake','x2':'X2'};\n",
    "\n",
    "output_stations = sorted(output_stations,key=lambda x: locs[x])\n",
    "output_stations = [abbrev_map[x] if x in abbrev_map.keys() else x for x in output_stations]\n",
    "\n",
    "pred = [lead_day_for_station[station] for station in output_stations]\n",
    "\n",
    "\n",
    "input_var_map = {'SAC':'SAC 0', 'Exp':'Exports',\n",
    "                 'SJR':'SJR','DICU':'DICU','Vern':'Vern EC',\n",
    "                 'SF_Tide':'SF_Tide', 'DXC':'DXC'}\n",
    "input_var_order = {'SAC 0':0, 'Exports':1,\n",
    "                 'SJR':2,'DICU':3,'Vern EC':4,\n",
    "                 'SF_Tide':5, 'DXC':6}\n",
    "\n",
    "try:\n",
    "    input_var = [input_var_map[var] for var in input_var]\n",
    "except Exception as e:\n",
    "    print('Input variables can only be selected in: SAC, Exp, SJR, DICU, Vern, SF_Tide, DXC.')\n",
    "    print('But got %s.' % e)\n",
    "\n",
    "sorted(input_var,key=lambda x: input_var_order[x])\n",
    "\n",
    "\n",
    "# set same weight for all stations\n",
    "class_weights = np.ones(12)\n",
    "\n",
    "\n",
    "# read data from csv files\n",
    "x_data,y_data=read_csv(input_data_path,output_data_path,input_var,output_stations)\n",
    "\n",
    "# normalize data to 0.1 ~ 0.9\n",
    "[x_data,x_slope,x_bias] = normalize_in(x_data)\n",
    "[y_data,y_slope,y_bias] = normalize_in(y_data)\n",
    "\n",
    "# prepare input and output data sample pairs\n",
    "x_data,y_data=process_data_vary_pred(x_data,y_data,single_daily+window_num*window_width,0,0,predict_list=pred)\n",
    "\n",
    "# split 80% data for training, 20% for testing\n",
    "x_train_ori, x_test_ori, y_train0, y_test0 = train_test_split(x_data,\n",
    "                                                              y_data,\n",
    "                                                              test_size=0.2,\n",
    "                                                              random_state = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCgG6f-kqbRS"
   },
   "source": [
    "# Define hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsO6AOfrfuyV"
   },
   "outputs": [],
   "source": [
    "# Adam optimizer settings\n",
    "batch_size = 32\n",
    "\n",
    "train_loc = locs[output_stations[0]]\n",
    "ann_name = '_'.join([abbrev_map[x.lower()][:4] if x.lower() in abbrev_map.keys() else x[:4] for x in output_stations])\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "if test_mode:\n",
    "    x_train_ori = x_train_ori[:100]\n",
    "    x_test_ori = x_test_ori[:100]\n",
    "    y_train0 = y_train0[:100]\n",
    "    y_test0 = y_test0[:100]\n",
    "    epochs = 5\n",
    "else:\n",
    "    epochs=200\n",
    "    \n",
    "train_err = []\n",
    "test_err = []\n",
    "train_shape = len(x_train_ori)\n",
    "\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch/epochs > 0.9:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch/epochs > .8:\n",
    "        lr *= 1e-3\n",
    "    elif epoch/epochs > .6:\n",
    "        lr *= 1e-2\n",
    "    elif epoch/epochs > .4:\n",
    "        lr *= 1e-1\n",
    "    elif epoch/epochs < .05:\n",
    "        lr *= epoch/5\n",
    "    if epoch/epochs < .05:\n",
    "        print('Learning rate: ', lr)\n",
    "    elif epoch/epochs in [0.9, 0.8, 0.6, 0.4]:\n",
    "        print('Learning rate changed to: ', lr)\n",
    "    return lr\n",
    "\n",
    "\n",
    "# custom loss function\n",
    "def weighted_mse(class_weights):\n",
    "    def custom_mse(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        # print('y_pred:', tf.keras.backend.int_shape(y_pred))\n",
    "        # print('y_true:', K.int_shape(y_true))\n",
    "        # calculating mean squared error\n",
    "        mse = tf.keras.backend.mean(class_weights * tf.keras.backend.square(y_pred - y_true), axis=-1)\n",
    "        return mse\n",
    "    return custom_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDmw5yGSqv32"
   },
   "source": [
    "# Build and train ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGftGCWXqZ6q"
   },
   "outputs": [],
   "source": [
    "# Build model\n",
    "conv_filter_init = tf.constant_initializer(conv_filter_generator(single_days=single_daily,window_num=window_num,window_size = window_width))\n",
    "\n",
    "inputs = tf.keras.Input(shape=(len(input_var),single_daily+window_num*window_width))\n",
    "x = tf.keras.layers.Conv1D(single_daily+window_num,1, activation='relu',\n",
    "                           kernel_initializer=conv_filter_init,\n",
    "                           kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0))(inputs)\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "for nodes in hidden_layer_nodes:\n",
    "    x = tf.keras.layers.Dense(nodes, activation='sigmoid')(x)\n",
    "x = tf.keras.layers.Dense(output_shape)(x)\n",
    "outputs = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Determine optimizer, loss function and evaluation metrics for the ANN\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=weighted_mse(class_weights),\n",
    "    metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define callbacks\n",
    "google_drive_ckpt_path = os.path.join(google_drive_dir,\"models/Conv_MTL/%s\"%ann_name)\n",
    "if running_on_colab:\n",
    "    checkpoint_filepath = os.path.join('/content/drive','My Drive',google_drive_ckpt_path,'model.ckpt')\n",
    "else:\n",
    "    checkpoint_filepath = os.path.join(google_drive_ckpt_path,'model.ckpt')\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
    "                                       save_weights_only=True,\n",
    "                                       monitor='val_loss',mode='min',save_best_only=True)\n",
    "    ]\n",
    "\n",
    "# Train the model\n",
    "start = time.time()\n",
    "history_callback=model.fit(x_train_ori,y_train0,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test_ori,y_test0),\n",
    "          callbacks=callbacks,verbose=1)\n",
    "end = time.time()\n",
    "print('Training finished in %d seconds' % (end-start) )\n",
    "print('Model saved to:', google_drive_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S42SqIHBssBM"
   },
   "source": [
    "# Print results in terms of mean squared error (MSE) and mean absolute percentage error (MAPE) on training and test sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ctd2V5Mercc"
   },
   "outputs": [],
   "source": [
    "print(\"With trainable conv layer (initialized with pre-determined weights):\\n\")\n",
    "\n",
    "y_train_pred = np.clip(model.predict(x_train_ori),0,1)\n",
    "y_test_pred = np.clip(model.predict(x_test_ori),0,1)\n",
    "\n",
    "cell_width = 15\n",
    "header_format = \"|{:^%d}|{:^%d}|{:^%d}|{:^%d}|{:^%d}|\" % (cell_width, cell_width, cell_width, cell_width, cell_width)\n",
    "row_format = \"|{:<%d}|{:<%d.1f}|{:<%d.2%%}|{:<%d.1f}|{:<%d.2%%}|\" % (cell_width, cell_width, cell_width, cell_width, cell_width)\n",
    "\n",
    "for ii in range(output_shape):\n",
    "    if ii == 0:\n",
    "        print(\"-\"*(cell_width*5+6))\n",
    "        print(header_format.format('Station', 'Train MSE', 'Train MAPE', 'Test MSE', 'Test MAPE'))  # *row == row element wise\n",
    "        print(\"-\"*(cell_width*5+6))\n",
    "        print(\"-\"*(cell_width*5+6))\n",
    "\n",
    "    train_mse = np.mean(((y_train_pred[:,ii]-y_train0[:,ii])/y_slope[ii])**2)\n",
    "    train_mape = np.mean(abs(y_train_pred[:,ii]-y_train0[:,ii])/y_train0[:,ii])\n",
    "    test_mse = np.mean(((y_test_pred[:,ii]-y_test0[:,ii])/y_slope[ii])**2)\n",
    "    test_mape = np.mean(abs(y_test_pred[:,ii]-y_test0[:,ii])/y_test0[:,ii])\n",
    "\n",
    "    print(row_format.format(output_stations[ii],train_mse, train_mape, test_mse, test_mape))\n",
    "    print(\"-\"*(cell_width*5+6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kneYn-cWLz5k"
   },
   "source": [
    "# Save true output data and ANN estimations into two separate text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDnSiKOC6tLZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "if running_on_colab:\n",
    "    results_dir = os.path.join('/content/drive','My Drive',google_drive_ckpt_path)\n",
    "else:\n",
    "    results_dir = google_drive_ckpt_path\n",
    "\n",
    "input_data = x_data\n",
    "output_data = y_data\n",
    "date = np.arange('1940-10',\n",
    "                 np.datetime64('1940-10') + np.timedelta64(len(input_data), 'D'),\n",
    "                 dtype='datetime64[D]')\n",
    "\n",
    "# let ANN compute estimations\n",
    "y_predicted = model.predict(input_data)\n",
    "\n",
    "# print estimation errors\n",
    "MSE = np.mean(((y_predicted-output_data)/y_slope)**2,axis=0)\n",
    "MAPE = np.mean(np.abs(y_predicted-output_data)/output_data,axis=0)\n",
    "\n",
    "# write ANN estimations to csv\n",
    "results = pd.DataFrame(data=((y_predicted-y_bias)/y_slope).reshape(-1,output_shape),\n",
    "                        index=date,\n",
    "                        columns=output_stations)\n",
    "results.index.name='date    '\n",
    "results.to_csv(os.path.join(results_dir,'ANN_estimations.txt'),\n",
    "                sep='\\t',\n",
    "                float_format='%5.4f',\n",
    "                header=True,\n",
    "                index=True)\n",
    "\n",
    "# write target output values to csv\n",
    "real_data = pd.DataFrame(data=(output_data/y_slope),\n",
    "                        index=date,\n",
    "                        columns=output_stations)\n",
    "real_data.index.name='date    '\n",
    "real_data.to_csv(os.path.join(results_dir,'target_outputs.txt'),\n",
    "                  sep='\\t',\n",
    "                  float_format='%5.4f',\n",
    "                  header=True,\n",
    "                  index=True)\n",
    "\n",
    "# print paths of the csv files\n",
    "print('-'*65)\n",
    "print('-'*65)\n",
    "print(\"ANN estimations written to: \\n%s\" %(os.path.abspath(os.path.join(results_dir,'ANN_estimations.txt'))))\n",
    "print('-'*65)\n",
    "print(\"True salinity values written to: \\n %s\" %(os.path.abspath(os.path.join(results_dir,'target_outputs.txt'))))\n",
    "print('-'*65)\n",
    "print('-'*65)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_MTL_Conv_ANN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
