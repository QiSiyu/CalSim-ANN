{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of train_multi_output_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKLwNexwgmCT",
        "colab_type": "text"
      },
      "source": [
        "## Training a 4-Output ANN from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcg-Adtqvzzh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fc4a0bdd-554f-4851-8728-b725c317d314"
      },
      "source": [
        "######## User Settings ########\n",
        "\n",
        "#### 1. Select parameters to be used ####\n",
        "input_var = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
        "#### 2. Select stations to be predicted ####\n",
        "# choose within 'Emmaton','Jersey Point','Collinsville','Rock Slough',\n",
        "# 'Antioch', 'Mallard', 'LosVaqueros', 'Martinez', 'MiddleRiver',\n",
        "# 'Vict Intake', 'CVP Intake', 'CCFB_OldR'\n",
        "output_stations=['Emmaton','Jersey Point','Collinsville','Rock Slough','Antioch']\n",
        "#### 3. Specify directory to excel dataset and the helper script (folder name only) ####\n",
        "google_drive_dir = 'python_ANN'\n",
        "\n",
        "###### User Settings Finished ######\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "data_in_google_drive = True\n",
        "\n",
        "# Mount Google drive\n",
        "if data_in_google_drive:\n",
        "    drive.mount('/content/drive')\n",
        "    xl_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"ANN_data2.xlsx\")\n",
        "    %tensorflow_version 1.x\n",
        "else:\n",
        "    xl_path = \"ANN_data2.xlsx\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu7ID-UUwBri",
        "colab_type": "text"
      },
      "source": [
        "##1. Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATT5MJ5NzQzw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir))\n",
        "from ann_helper import normalize_in,read_data,writeF90,initnw,show_eval"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6348s2lzcfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f11a227d-4bd6-4caf-968d-f6ec01c40cd6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "# from sklearn.utils import shuffle\n",
        "\n",
        "test_mode = False\n",
        "\n",
        "locs = {'Emmaton':0,'Jersey Point':1,'Collinsville':2,'Rock Slough':3,\n",
        "        'Antioch':4,'Mallard':5,'LosVaqueros':6, 'Martinez':7,\n",
        "        'MiddleRiver':8,'Vict Intake':9,'CVP Intake':10,'CCFB_OldR':11}\n",
        "abbrev_map = {'rock slough':'ORRSL','rockslough':'ORRSL',\n",
        "            'emmaton':'EMM','jersey point':'JP','jerseypoint':'JP',\n",
        "            'antioch':'antioch','collinsville':'CO',\n",
        "            'mallard':'Mallard','mallard island':'Mallard',\n",
        "            'los vaqueros':'LosVaqueros','losvaqueros':'LosVaqueros',\n",
        "            'martinez':'MTZ',\n",
        "            'middle river':'MidR_intake','MiddleRiver':'MidR_intake',\n",
        "            'victoria cannal':'Victoria_intake','Vict Intake':'Victoria_intake',\n",
        "            'cvp intake':'CVP_intake','clfct forebay':'CCFB',\n",
        "            'clfct forebay intake':'CCFB_intake','x2':'X2'};\n",
        "\n",
        "output_stations = sorted(output_stations,key=lambda x: locs[x])\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "#   print('xxxxxxxxxxxxxx Using CPU xxxxxxxxxxxxxx')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "output_shape = len(output_stations)\n",
        "ann_name = '_'.join(map(lambda x: abbrev_map[x.lower()],\n",
        "                        output_stations))\n",
        "nn_shape = [None,8*output_shape,2*output_shape,output_shape]\n",
        "# input_shape = (1,17*len(input_var))\n",
        "on_server = True\n",
        "\n",
        "\n",
        "max_fail = 3\n",
        "epochs = 100\n",
        "init_mu = .05\n",
        "mu_max = 1e10\n",
        "target_mse = 0.\n",
        "\n",
        "# adam settings\n",
        "adam_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "print('Test mode is set to: ',test_mode)\n",
        "print('current device is: ',device_name)\n",
        "\n",
        "train_loc = np.arange(output_shape)\n",
        "\n",
        "x_data,y_data = read_data(xl_path,input_var,output_stations)\n",
        "\n",
        "[x_norm,x_slope,x_bias] = normalize_in(x_data)\n",
        "[y_norm,y_slope,y_bias] = normalize_in(y_data)\n",
        "\n",
        "x_train_ori, x_test_ori, y_train0, y_test0 = train_test_split(x_norm,\n",
        "                                                              y_norm,\n",
        "                                                              test_size=0.2,\n",
        "                                                              random_state = 0)\n",
        "print('train set input shape is: ',x_train_ori.shape)\n",
        "print('train set output shape is: ',y_train0.shape)\n",
        "\n",
        "train_err = []\n",
        "test_err = []\n",
        "train_shape = len(x_train_ori)\n",
        "\n",
        "nn_shape[0] = x_train_ori.shape[1]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Test mode is set to:  False\n",
            "current device is:  /device:GPU:0\n",
            "Disgarding last 1 row(s) of data in output set...\n",
            "loading data in 48.75 seconds\n",
            "train set input shape is:  (21913, 126)\n",
            "train set output shape is:  (21913, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZNeDaHG0fyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef7ebd51-b738-4032-eafd-7fb85fea3354"
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, nn_shape[0]], name='InputData')\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, output_shape], name='LabelData')\n",
        "\n",
        "init_val = initnw(list(zip(*(nn_shape[i:] for i in range(2)))),x_train_ori)\n",
        "\n",
        "W1 = tf.Variable(initial_value=init_val[0][0], name='w1',dtype='float32')\n",
        "b1 = tf.Variable(initial_value=init_val[0][1], name='b1',dtype='float32')\n",
        "W2 = tf.Variable(initial_value=init_val[1][0], name='w2',dtype='float32')\n",
        "b2 = tf.Variable(initial_value=init_val[1][1], name='b2',dtype='float32')\n",
        "W3 = tf.Variable(initial_value=init_val[2][0], name='w3',dtype='float32')\n",
        "b3 = tf.Variable(initial_value=init_val[2][1], name='b3',dtype='float32')\n",
        "\n",
        "with tf.compat.v1.name_scope('layer1'):\n",
        "    first_out = tf.sigmoid(tf.add(tf.matmul(x,W1),b1))\n",
        "with tf.compat.v1.name_scope('layer2'):\n",
        "    second_out = tf.sigmoid(tf.add(tf.matmul(first_out,W2),b2))\n",
        "# with tf.name_scope('layer3'):\n",
        "#     third_out = tf.sigmoid(tf.add(tf.matmul(second_out,W3),b3))\n",
        "with tf.compat.v1.name_scope('layer3'):\n",
        "    pred = tf.matmul(second_out, W3) + b3\n",
        "\n",
        "# residuals\n",
        "r = tf.subtract(pred,y)\n",
        "\n",
        "# loss function must be mse or sse for LM\n",
        "cost = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "\n",
        "# >> For adam optimizer:\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "initial_lr = 1e-2\n",
        "learning_rate = tf.compat.v1.train.exponential_decay(initial_lr, global_step,\n",
        "                                       40, 0.999, staircase=True)\n",
        "learning_rate2 = tf.compat.v1.train.exponential_decay(initial_lr/20, global_step,\n",
        "                                       40, 0.999, staircase=True)\n",
        "\n",
        "# Adam optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    adam_opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step = global_step)\n",
        "    adam_opt2 = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate2).minimize(cost,global_step = global_step)\n",
        "\n",
        "# LM optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=1)\n",
        "with tf.compat.v1.name_scope('Accuracy'):\n",
        "    # Accuracy\n",
        "    acc = tf.reduce_mean(input_tensor=tf.abs(tf.square(tf.subtract(pred,y))))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "\n",
        "def jacobian(y, x):\n",
        "    loop_vars = [\n",
        "        tf.constant(0, tf.int32),\n",
        "        tf.TensorArray(tf.float32, size=train_shape*output_shape),\n",
        "    ]\n",
        "\n",
        "    _, jacobian = tf.while_loop(\n",
        "        cond=lambda i, _: i < train_shape*output_shape,\n",
        "        body=lambda i, res: (i+1, res.write(i, tf.reshape(tf.gradients(ys=y[i], xs=x), (-1,)))),\n",
        "        loop_vars=loop_vars)\n",
        "    return jacobian.stack()\n",
        "\n",
        "\n",
        "r_flat = tf.expand_dims(tf.reshape(r,[-1]), 1)\n",
        "parms = [W1, b1, W2, b2, W3, b3]\n",
        "parms_sizes = [tf.size(input=p) for p in parms]\n",
        "j = tf.concat([jacobian(r_flat, p) for p in parms], 1)\n",
        "jT = tf.transpose(a=j)\n",
        "hess_approx = tf.matmul(jT, j)\n",
        "grad_approx = tf.matmul(jT, r_flat)\n",
        "\n",
        "mu = tf.compat.v1.placeholder(tf.float32, shape=[])\n",
        "\n",
        "store = [tf.Variable(tf.zeros(p.shape, dtype=tf.float32)) for p in parms]\n",
        "save_parms = [tf.compat.v1.assign(s, p) for s, p in zip(store, parms)]\n",
        "restore_parms = [tf.compat.v1.assign(p, s) for s, p in zip(store, parms)]\n",
        "\n",
        "wb_flat = tf.concat([tf.reshape(p,[-1,1]) for p in parms],axis=0)\n",
        "n = tf.add_n(parms_sizes)\n",
        "I = tf.eye(n, dtype=tf.float32)\n",
        "w_2 = tf.reduce_sum(input_tensor=tf.square(wb_flat))\n",
        "\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(init)\n",
        "print(\"Number of trainable parameters: %d\" % sess.run(n))\n",
        "\n",
        "\n",
        "#  lm\n",
        "dp_flat = tf.matmul(tf.linalg.inv(hess_approx + tf.multiply(mu, I)), grad_approx)\n",
        "\n",
        "#  lm\n",
        "dps = tf.split(dp_flat, parms_sizes, 0)\n",
        "\n",
        "for i in range(len(dps)):\n",
        "    dps[i] = tf.reshape(dps[i], parms[i].shape)\n",
        "lm = opt.apply_gradients(zip(dps, parms))\n",
        "\n",
        "# dXs = tf.split(dX, parms_sizes, 0)\n",
        "\n",
        "# for i in range(len(dXs)):\n",
        "#     dXs[i] = tf.reshape(dXs[i], parms[i].shape)\n",
        "\n",
        "# br = opt.apply_gradients(zip(dXs, parms))\n",
        "\n",
        "\n",
        "\n",
        "## update alpha and beta\n",
        "# gamk = tf.cast(n,'float32') - alpha * tf.trace(tf.linalg.inv(tf.scalar_mul(beta,hess_approx)+tf.multiply(mu, I)))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 5545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBz9Pd-T0Zbi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.compat.v1.train.Saver({'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3})"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZmYeHlnEzG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adam optimizer training\n",
        "\n",
        "\n",
        "# Feed batch data\n",
        "def get_batch(inputX, inputY, batch_size):\n",
        "    duration = len(inputX)\n",
        "    for i in range(0,duration//batch_size):\n",
        "        idx = i*batch_size\n",
        "        yield inputX[idx:idx+batch_size], inputY[idx:idx+batch_size]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Start training\n",
        "\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        cost_list = []\n",
        "        lr_list = []\n",
        "        # Run the initializer\n",
        "        sess.run(init)\n",
        "\n",
        "        # Training cycle\n",
        "        total_batch = train_shape//batch_size\n",
        "        for epoch in range(adam_epochs):\n",
        "            avg_cost = 0.\n",
        "            ii = 0\n",
        "            # Loop over all batches\n",
        "            for batch_xs, batch_ys in get_batch(x_train_ori, y_train0,batch_size):\n",
        "                ii = ii +1\n",
        "                # Run optimization op (backprop), cost op (to get loss value)\n",
        "                # and summary nodes\n",
        "                _, c = sess.run([adam_opt, cost],\n",
        "                                 feed_dict={x: batch_xs, y: batch_ys})\n",
        "                # Write logs at every iteration\n",
        "                # Compute average loss\n",
        "                avg_cost += c / total_batch\n",
        "            lr = learning_rate.eval()\n",
        "            cost_list.append(avg_cost)\n",
        "            lr_list.append(lr)\n",
        "            # Display logs per epoch step\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost),\n",
        "                  \"lr = \",\"{:.10f}\".format(lr))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    # Calculate accuracy\n",
        "    print(\"Train Error:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "    print(\"Test Error:\", acc.eval({x: x_test_ori, y: y_test0}))\n",
        "    y_test_ = sess.run(pred,feed_dict={x:x_test_ori})\n",
        "    print(\"Test MSE:\", np.mean(np.square((y_test_-y_test0)/y_slope)))\n",
        "    print(\"Test MAPE:\", np.mean(np.abs(y_test_-y_test0)/y_test0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yOoG07oj65_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4aeffe36-5ef6-4744-a14a-d2a5fa700dca"
      },
      "source": [
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        print(\"Train Error before LM:\", acc.eval({x: x_train_ori, y: y_train0}))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Error before LM: 0.004383577\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FrIJ-IAfF9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24607cc6-e313-4a15-c4b6-b64e40495b24"
      },
      "source": [
        "\n",
        "feed_dict = {x: x_train_ori,\n",
        "             y: y_train0}\n",
        "feed_dict[mu] = init_mu\n",
        "# feed_dict[beta] = init_beta\n",
        "# feed_dict[alpha] = init_alpha\n",
        "\n",
        "\n",
        "# construct so-called feed dictionary to map placeholders to actual values\n",
        "\n",
        "validation_feed_dict = {x: x_test_ori,\n",
        "                        y: y_test0}\n",
        "\n",
        "train_break = False\n",
        "saver = tf.compat.v1.train.Saver({'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3})\n",
        "current_loss = sess.run(cost,{x: x_train_ori,y: y_train0})\n",
        "\n",
        "# Start training\n",
        "start = time.time()\n",
        "if not os.path.exists(os.path.join('/content/drive',\n",
        "                                   'My Drive',\n",
        "                                   google_drive_dir,\n",
        "                                   \"models\",\n",
        "                                   ann_name)):\n",
        "    os.makedirs(os.path.join('/content/drive',\n",
        "                                   'My Drive',\n",
        "                                   google_drive_dir,\n",
        "                                   \"models\",\n",
        "                                   ann_name))\n",
        "\n",
        "# current_br_loss = sess.run(br_loss,feed_dict)\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        epoch = 1\n",
        "        fail_step = 0\n",
        "        # Training cycle\n",
        "        while epoch < epochs and current_loss > target_mse:\n",
        "            val_loss = sess.run(cost, validation_feed_dict)\n",
        "            print('epoch: %3d , mu: %.5e, current loss: %.10f, val loss: %.10f'%(epoch,feed_dict[mu],current_loss,val_loss))\n",
        "            sess.run(save_parms)\n",
        "            while True:\n",
        "                start_step = time.time()\n",
        "                sess.run(lm, feed_dict)\n",
        "                # print('One update ended in %d seconds' % (time.time()-start_step))\n",
        "                new_loss = sess.run(cost,feed_dict)\n",
        "                if new_loss > current_loss:\n",
        "                    fail_step += 1\n",
        "                    feed_dict[mu] *= 10\n",
        "                    if feed_dict[mu] > mu_max or fail_step > max_fail:\n",
        "                        train_break = True\n",
        "                        break\n",
        "                    sess.run(restore_parms)\n",
        "                else:\n",
        "                    print(\"mu: %.5e, new cost: %.5f, best cost: %.5f\"%(feed_dict[mu],new_loss,current_loss))\n",
        "                    fail_step = 0\n",
        "                    feed_dict[mu] /= 10\n",
        "                    feed_dict[mu] = max(1e-20,feed_dict[mu])\n",
        "                    current_loss = new_loss\n",
        "                    break\n",
        "            if train_break:\n",
        "                print('Failed for %d step(s), current mu = %.5e, stop training' % (fail_step,feed_dict[mu]))\n",
        "                break\n",
        "            epoch += 1\n",
        "    end = time.time()\n",
        "    print(\"Optimization Finished in %d seconds!\" %(end - start))\n",
        "    save_path = saver.save(sess, os.path.join('/content/drive','My Drive',google_drive_dir,\"models/%s/model.ckpt\"%ann_name))\n",
        "    print(\"Model saved in path: %s\" % save_path)\n",
        "\n",
        "    # Test model\n",
        "    y_train_predicted = sess.run(pred,feed_dict)\n",
        "    y_test_predicted = sess.run(pred,validation_feed_dict)\n",
        "    for ii in range(output_shape):\n",
        "        show_eval(y_train_predicted[:,ii],\n",
        "                y_train0[:,ii],\n",
        "                y_test_predicted[:,ii],\n",
        "                y_test0[:,ii],\n",
        "                y_slope[ii],y_bias[ii],ann_name)\n",
        "    writeF90(os.path.join('/content/drive',\n",
        "                                   'My Drive',\n",
        "                                   google_drive_dir,\n",
        "                                   \"models\",\n",
        "                                   ann_name),\n",
        "             ann_name,y_slope[train_loc],y_bias[train_loc],\n",
        "             sess.run(W1).transpose(),sess.run(b1),\n",
        "             sess.run(W2).transpose(),sess.run(b2),\n",
        "             sess.run(W3).transpose(),sess.run(b3))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:   1 , mu: 5.00000e-02, current loss: 0.0043835770, val loss: 0.0044974596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTu3JF3VXM-5",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnGZRz9gkKGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adam again\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        print(\"Train Error after LM:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "\n",
        "# Start training\n",
        "\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        cost_list = []\n",
        "        lr_list = []\n",
        "        # Run the initializer\n",
        "#         sess.run(init)\n",
        "\n",
        "        # Training cycle\n",
        "        total_batch = train_shape//batch_size\n",
        "        for epoch in range(adam_epochs):\n",
        "            avg_cost = 0.\n",
        "            ii = 0\n",
        "            # Loop over all batches\n",
        "            for batch_xs, batch_ys in get_batch(x_train_ori, y_train0,batch_size):\n",
        "                ii = ii +1\n",
        "                # Run optimization op (backprop), cost op (to get loss value)\n",
        "                # and summary nodes\n",
        "                _, c = sess.run([adam_opt2, cost],\n",
        "                                 feed_dict={x: batch_xs, y: batch_ys})\n",
        "                # Write logs at every iteration\n",
        "                # Compute average loss\n",
        "                avg_cost += c / total_batch\n",
        "            lr = learning_rate.eval()\n",
        "            cost_list.append(avg_cost)\n",
        "            lr_list.append(lr)\n",
        "            # Display logs per epoch step\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost),\n",
        "                  \"lr = \",\"{:.10f}\".format(lr))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    # Calculate accuracy\n",
        "    print(\"Train Error:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "    print(\"Test Error:\", acc.eval({x: x_test_ori, y: y_test0}))\n",
        "    y_train_ = sess.run(pred,feed_dict={x:x_train_ori})\n",
        "    y_test_ = sess.run(pred,feed_dict={x:x_test_ori})\n",
        "    print(\"Train MSE:\", np.mean(np.square((y_train_-y_train0)/y_slope)))\n",
        "    print(\"Test MSE:\", np.mean(np.square((y_test_-y_test0)/y_slope)))\n",
        "    print(\"Train MAPE:\", np.mean(np.abs(y_train_-y_train0)/y_train0))\n",
        "    print(\"Test MAPE:\", np.mean(np.abs(y_test_-y_test0)/y_test0))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}