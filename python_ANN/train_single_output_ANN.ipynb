{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_single_output_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbLsjPUk3oIb",
        "colab_type": "text"
      },
      "source": [
        "## Training a Single-Output ANN from scratch\n",
        "\n",
        "Note: this script trains only one ANN for the station selected by user. Please set 'output_stations' to different stations for training multiple ANNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHPnGTMf95oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## User Settings ########\n",
        "\n",
        "#### 1. Select parameters to be used ####\n",
        "input_var = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
        "#### 2. Select stations to be predicted ####\n",
        "# choose ONE of 'Emmaton','Jersey Point','Collinsville','Rock Slough'\n",
        "output_stations=['Rock Slough']\n",
        "#### 3. Specify directory to excel dataset and the helper script (folder name only) ####\n",
        "google_drive_dir = 'calsim_ANN'\n",
        "\n",
        "###### User Settings Finished ######\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "# Mount Google drive\n",
        "data_in_google_drive = True\n",
        "\n",
        "if data_in_google_drive:\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    xl_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"ANN_data.xlsx\")\n",
        "else:\n",
        "    xl_path = \"ANN_data.xlsx\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Wd7z3l996j",
        "colab_type": "text"
      },
      "source": [
        "##1. Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6GxpZSUj0dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import helper functions\n",
        "import sys\n",
        "sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir))\n",
        "from ann_helper import read_data,normalize_in,writeF90,initnw,show_eval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6348s2lzcfC",
        "colab_type": "code",
        "outputId": "5c01fe62-9940-49fa-e1c6-5755a6d7a187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "# from sklearn.utils import shuffle\n",
        "from scipy import stats\n",
        "\n",
        "test_mode = False\n",
        "\n",
        "locs = {'Emmaton':0,'Jersey Point':1,'Collinsville':2,'Rock Slough':3}\n",
        "abbrev_map = {'rock slough':'ORRSL','rockslough':'ORRSL',\n",
        "            'emmaton':'EMM','jersey point':'JP','jerseypoint':'JP',\n",
        "            'antioch':'antioch','collinsville':'CO',\n",
        "            'mallard':'Mallard','mallard island':'Mallard',\n",
        "            'los vaqueros':'LosVaqueros','losvaqueros':'LosVaqueros',\n",
        "            'martinez':'MTZ',\n",
        "            'middle river':'MidR_intake','MiddleRiver':'MidR_intake',\n",
        "            'victoria cannal':'Victoria_intake','Vict Intake':'Victoria_intake',\n",
        "            'cvp intake':'CVP_intake','clfct forebay':'CCFB',\n",
        "            'clfct forebay intake':'CCFB_intake','x2':'X2'};\n",
        "\n",
        "output_stations = sorted(output_stations,key=lambda x: locs[x])\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "#   print('xxxxxxxxxxxxxx Using CPU xxxxxxxxxxxxxx')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "input_shape = (1,17*len(input_var))\n",
        "output_shape = 1\n",
        "nn_shape = [17*len(input_var),8,2,1] # number of neurons in each layer\n",
        "on_server = True\n",
        "\n",
        "# LM optimizer settings\n",
        "max_fail = 3\n",
        "epochs = 10\n",
        "init_mu = .05\n",
        "mu_max = 1e10\n",
        "target_mse = 0.\n",
        "\n",
        "# adam optimizer settings\n",
        "adam_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "if test_mode or device_name != '/device:GPU:0':\n",
        "    epochs = 5\n",
        "else:\n",
        "    epochs = 100\n",
        "\n",
        "train_loc = locs[output_stations[0]]\n",
        "ann_name = abbrev_map[output_stations[0].lower()]\n",
        "start = time.time()\n",
        "\n",
        "# read data from excel\n",
        "x_data,y_data = read_data(xl_path,input_var,output_stations)\n",
        "end = time.time()\n",
        "print(\"loading data in %.2f seconds\" % (end-start) )\n",
        "    \n",
        "# normalize data to 0.1 ~ 0.9\n",
        "[x_norm,x_slope,x_bias] = normalize_in(x_data)\n",
        "[y_norm,y_slope,y_bias] = normalize_in(y_data)\n",
        "\n",
        "# split 80% for training, 20% for testing\n",
        "x_train_ori, x_test_ori, y_train0, y_test0 = train_test_split(x_norm,\n",
        "                                                              y_norm,\n",
        "                                                              test_size=0.2,\n",
        "                                                              random_state = 0)\n",
        "\n",
        "if test_mode:\n",
        "    x_train_ori = x_train_ori[:100]\n",
        "    x_test_ori = x_test_ori[:100]\n",
        "    y_train0 = y_train0[:100]\n",
        "    y_test0 = y_test0[:100]\n",
        "train_err = []\n",
        "test_err = []\n",
        "train_shape = len(x_train_ori)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Disgarding last 1 row(s) of data in output set...\n",
            "loading data in 27.36 seconds\n",
            "loading data in 27.39 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZNeDaHG0fyp",
        "colab_type": "code",
        "outputId": "584dbb7e-34f1-493b-f300-b31820a25186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# build artificial neural network\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, 17*len(input_var)], name='InputData')\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, 1], name='LabelData')\n",
        "\n",
        "init_val = initnw(list(zip(*(nn_shape[i:] for i in range(2)))),x_train_ori)\n",
        "\n",
        "W1 = tf.Variable(initial_value=init_val[0][0], name='w1',dtype='float32')\n",
        "b1 = tf.Variable(initial_value=init_val[0][1], name='b1',dtype='float32')\n",
        "W2 = tf.Variable(initial_value=init_val[1][0], name='w2',dtype='float32')\n",
        "b2 = tf.Variable(initial_value=init_val[1][1], name='b2',dtype='float32')\n",
        "W3 = tf.Variable(initial_value=init_val[2][0], name='w3',dtype='float32')\n",
        "b3 = tf.Variable(initial_value=init_val[2][1], name='b3',dtype='float32')\n",
        "\n",
        "with tf.compat.v1.name_scope('layer1'):\n",
        "    first_out = tf.sigmoid(tf.add(tf.matmul(x,W1),b1))\n",
        "with tf.compat.v1.name_scope('layer2'):\n",
        "    second_out = tf.sigmoid(tf.add(tf.matmul(first_out,W2),b2))\n",
        "# with tf.name_scope('layer3'):\n",
        "#     third_out = tf.sigmoid(tf.add(tf.matmul(second_out,W3),b3))\n",
        "with tf.compat.v1.name_scope('layer3'):\n",
        "    pred = tf.matmul(second_out, W3) + b3\n",
        "\n",
        "# residuals\n",
        "r = tf.subtract(pred,y)\n",
        "# loss function must be mse or sse for LM\n",
        "cost = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "\n",
        "# For adam optimizer:\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "initial_lr = 1e-2\n",
        "learning_rate = tf.compat.v1.train.exponential_decay(initial_lr, global_step,\n",
        "                                       40, 0.999, staircase=True)\n",
        "# Adam optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    adam_opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step = global_step)\n",
        "\n",
        "# LM optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=1)\n",
        "\n",
        "with tf.compat.v1.name_scope('Accuracy'):\n",
        "    # Accuracy\n",
        "    acc = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "\n",
        "# LM algorithm\n",
        "def jacobian(y, x):\n",
        "    loop_vars = [\n",
        "        tf.constant(0, tf.int32),\n",
        "        tf.TensorArray(tf.float32, size=train_shape),\n",
        "    ]\n",
        "\n",
        "    _, jacobian = tf.while_loop(\n",
        "        cond=lambda i, _: i < train_shape,\n",
        "        body=lambda i, res: (i+1, res.write(i, tf.reshape(tf.gradients(ys=y[i], xs=x), (-1,)))),\n",
        "        loop_vars=loop_vars)\n",
        "    return jacobian.stack()\n",
        "\n",
        "\n",
        "r_flat = tf.expand_dims(tf.reshape(r,[-1]),1)\n",
        "parms = [W1, b1, W2, b2, W3, b3]\n",
        "parms_sizes = [tf.size(input=p) for p in parms]\n",
        "j = tf.concat([jacobian(r_flat, p) for p in parms], 1)\n",
        "jT = tf.transpose(a=j)\n",
        "hess_approx = tf.matmul(jT, j)\n",
        "grad_approx = tf.matmul(jT, r_flat)\n",
        "\n",
        "mu = tf.compat.v1.placeholder(tf.float32, shape=[])\n",
        "\n",
        "store = [tf.Variable(tf.zeros(p.shape, dtype=tf.float32)) for p in parms]\n",
        "save_parms = [tf.compat.v1.assign(s, p) for s, p in zip(store, parms)]\n",
        "restore_parms = [tf.compat.v1.assign(p, s) for s, p in zip(store, parms)]\n",
        "\n",
        "wb_flat = tf.concat([tf.reshape(p,[-1,1]) for p in parms],axis=0)\n",
        "n = tf.add_n(parms_sizes)\n",
        "I = tf.eye(n, dtype=tf.float32)\n",
        "w_2 = tf.reduce_sum(input_tensor=tf.square(wb_flat))\n",
        "\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(init)\n",
        "print(\"Number of trainable parameters: %d\" % sess.run(n))\n",
        "\n",
        "\n",
        "#  lm algorithm\n",
        "dp_flat = tf.matmul(tf.linalg.inv(hess_approx + tf.multiply(mu, I)), grad_approx)\n",
        "dps = tf.split(dp_flat, parms_sizes, 0)\n",
        "\n",
        "for i in range(len(dps)):\n",
        "    dps[i] = tf.reshape(dps[i], parms[i].shape)\n",
        "lm = opt.apply_gradients(zip(dps, parms))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgyRTmYsLR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.compat.v1.train.Saver({'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy1XKPUeKd10",
        "colab_type": "code",
        "outputId": "b29ecffa-e740-4a18-c2dd-adec806e2a22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Adam optimizer training\n",
        "\n",
        "\n",
        "# Feed batch data\n",
        "def get_batch(inputX, inputY, batch_size):\n",
        "    duration = len(inputX)\n",
        "    for i in range(0,duration//batch_size):\n",
        "        idx = i*batch_size\n",
        "        yield inputX[idx:idx+batch_size], inputY[idx:idx+batch_size]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Start training\n",
        "\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        cost_list = []\n",
        "        lr_list = []\n",
        "        # Run the initializer\n",
        "        sess.run(init)\n",
        "\n",
        "        # Training cycle\n",
        "        total_batch = train_shape//batch_size\n",
        "        for epoch in range(adam_epochs):\n",
        "            avg_cost = 0.\n",
        "            ii = 0\n",
        "            # Loop over all batches\n",
        "            for batch_xs, batch_ys in get_batch(x_train_ori, y_train0,batch_size):\n",
        "                ii = ii +1\n",
        "                # Run optimization op (backprop), cost op (to get loss value)\n",
        "                # and summary nodes\n",
        "                _, c = sess.run([adam_opt, cost],\n",
        "                                 feed_dict={x: batch_xs, y: batch_ys})\n",
        "                # Write logs at every iteration\n",
        "                # Compute average loss\n",
        "                avg_cost += c / total_batch\n",
        "            lr = learning_rate.eval()\n",
        "            cost_list.append(avg_cost)\n",
        "            lr_list.append(lr)\n",
        "            # Display logs per epoch step\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost),\n",
        "                  \"lr = \",\"{:.10f}\".format(lr))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    # Calculate accuracy\n",
        "    print(\"Train Error:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "    print(\"Test Error:\", acc.eval({x: x_test_ori, y: y_test0}))\n",
        "    y_test_ = sess.run(pred,feed_dict={x:x_test_ori})\n",
        "    print(\"Test MSE:\", np.mean(np.square((y_test_-y_test0)/y_slope)))\n",
        "    print(\"Test MAPE:\", np.mean(np.abs(y_test_-y_test0)/y_test0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 0.237069444 lr =  0.0098313550\n",
            "Epoch: 0002 cost= 0.008053839 lr =  0.0096655544\n",
            "Epoch: 0003 cost= 0.003651650 lr =  0.0095025497\n",
            "Epoch: 0004 cost= 0.003204303 lr =  0.0093422951\n",
            "Epoch: 0005 cost= 0.002972378 lr =  0.0091847414\n",
            "Epoch: 0006 cost= 0.002673858 lr =  0.0090298457\n",
            "Epoch: 0007 cost= 0.002483110 lr =  0.0088775624\n",
            "Epoch: 0008 cost= 0.002382263 lr =  0.0087278476\n",
            "Epoch: 0009 cost= 0.002309353 lr =  0.0085806567\n",
            "Epoch: 0010 cost= 0.002249185 lr =  0.0084275128\n",
            "Epoch: 0011 cost= 0.002196273 lr =  0.0082853874\n",
            "Epoch: 0012 cost= 0.002149378 lr =  0.0081456583\n",
            "Epoch: 0013 cost= 0.002107978 lr =  0.0080082864\n",
            "Epoch: 0014 cost= 0.002069965 lr =  0.0078732315\n",
            "Epoch: 0015 cost= 0.002035243 lr =  0.0077404534\n",
            "Epoch: 0016 cost= 0.002002807 lr =  0.0076099150\n",
            "Epoch: 0017 cost= 0.001972220 lr =  0.0074815778\n",
            "Epoch: 0018 cost= 0.001945806 lr =  0.0073554050\n",
            "Epoch: 0019 cost= 0.001915558 lr =  0.0072313598\n",
            "Epoch: 0020 cost= 0.001889187 lr =  0.0071022981\n",
            "Epoch: 0021 cost= 0.001864011 lr =  0.0069825212\n",
            "Epoch: 0022 cost= 0.001839900 lr =  0.0068647647\n",
            "Epoch: 0023 cost= 0.001816803 lr =  0.0067489943\n",
            "Epoch: 0024 cost= 0.001794634 lr =  0.0066351760\n",
            "Epoch: 0025 cost= 0.001773261 lr =  0.0065232776\n",
            "Epoch: 0026 cost= 0.001752863 lr =  0.0064132661\n",
            "Epoch: 0027 cost= 0.001731779 lr =  0.0063051092\n",
            "Epoch: 0028 cost= 0.001713770 lr =  0.0061987774\n",
            "Epoch: 0029 cost= 0.001695020 lr =  0.0060942383\n",
            "Epoch: 0030 cost= 0.001676932 lr =  0.0059854710\n",
            "Epoch: 0031 cost= 0.001659315 lr =  0.0058845291\n",
            "Epoch: 0032 cost= 0.001640523 lr =  0.0057852892\n",
            "Epoch: 0033 cost= 0.001628523 lr =  0.0056877243\n",
            "Epoch: 0034 cost= 0.001609450 lr =  0.0055918032\n",
            "Epoch: 0035 cost= 0.001583327 lr =  0.0054975008\n",
            "Epoch: 0036 cost= 0.001559412 lr =  0.0054047885\n",
            "Epoch: 0037 cost= 0.001540261 lr =  0.0053136395\n",
            "Epoch: 0038 cost= 0.001514622 lr =  0.0052240281\n",
            "Epoch: 0039 cost= 0.001486894 lr =  0.0051359269\n",
            "Epoch: 0040 cost= 0.001466815 lr =  0.0050442643\n",
            "Epoch: 0041 cost= 0.001449782 lr =  0.0049591945\n",
            "Epoch: 0042 cost= 0.001431837 lr =  0.0048755603\n",
            "Epoch: 0043 cost= 0.001414667 lr =  0.0047933366\n",
            "Epoch: 0044 cost= 0.001398825 lr =  0.0047124997\n",
            "Epoch: 0045 cost= 0.001384466 lr =  0.0046330262\n",
            "Epoch: 0046 cost= 0.001369505 lr =  0.0045548924\n",
            "Epoch: 0047 cost= 0.001355774 lr =  0.0044780769\n",
            "Epoch: 0048 cost= 0.001342747 lr =  0.0044025565\n",
            "Epoch: 0049 cost= 0.001329524 lr =  0.0043283100\n",
            "Epoch: 0050 cost= 0.001317461 lr =  0.0042510596\n",
            "Epoch: 0051 cost= 0.001304194 lr =  0.0041793678\n",
            "Epoch: 0052 cost= 0.001290374 lr =  0.0041088853\n",
            "Epoch: 0053 cost= 0.001276464 lr =  0.0040395907\n",
            "Epoch: 0054 cost= 0.001260784 lr =  0.0039714654\n",
            "Epoch: 0055 cost= 0.001243110 lr =  0.0039044886\n",
            "Epoch: 0056 cost= 0.001228258 lr =  0.0038386418\n",
            "Epoch: 0057 cost= 0.001214625 lr =  0.0037739053\n",
            "Epoch: 0058 cost= 0.001201194 lr =  0.0037102599\n",
            "Epoch: 0059 cost= 0.001187866 lr =  0.0036476888\n",
            "Epoch: 0060 cost= 0.001174664 lr =  0.0035825865\n",
            "Epoch: 0061 cost= 0.001161661 lr =  0.0035221679\n",
            "Epoch: 0062 cost= 0.001148844 lr =  0.0034627684\n",
            "Epoch: 0063 cost= 0.001136120 lr =  0.0034043705\n",
            "Epoch: 0064 cost= 0.001123208 lr =  0.0033469577\n",
            "Epoch: 0065 cost= 0.001109518 lr =  0.0032905131\n",
            "Epoch: 0066 cost= 0.001094932 lr =  0.0032350207\n",
            "Epoch: 0067 cost= 0.001080364 lr =  0.0031804633\n",
            "Epoch: 0068 cost= 0.001066093 lr =  0.0031268266\n",
            "Epoch: 0069 cost= 0.001052155 lr =  0.0030740942\n",
            "Epoch: 0070 cost= 0.001038885 lr =  0.0030192290\n",
            "Epoch: 0071 cost= 0.001026628 lr =  0.0029683115\n",
            "Epoch: 0072 cost= 0.001015396 lr =  0.0029182525\n",
            "Epoch: 0073 cost= 0.001005045 lr =  0.0028690377\n",
            "Epoch: 0074 cost= 0.000995419 lr =  0.0028206529\n",
            "Epoch: 0075 cost= 0.000986403 lr =  0.0027730847\n",
            "Epoch: 0076 cost= 0.000977886 lr =  0.0027263178\n",
            "Epoch: 0077 cost= 0.000969824 lr =  0.0026803399\n",
            "Epoch: 0078 cost= 0.000962161 lr =  0.0026351374\n",
            "Epoch: 0079 cost= 0.000954828 lr =  0.0025906970\n",
            "Epoch: 0080 cost= 0.000947801 lr =  0.0025444597\n",
            "Epoch: 0081 cost= 0.000941039 lr =  0.0025015485\n",
            "Epoch: 0082 cost= 0.000934508 lr =  0.0024593612\n",
            "Epoch: 0083 cost= 0.000928200 lr =  0.0024178852\n",
            "Epoch: 0084 cost= 0.000922074 lr =  0.0023771089\n",
            "Epoch: 0085 cost= 0.000916132 lr =  0.0023370204\n",
            "Epoch: 0086 cost= 0.000910333 lr =  0.0022976077\n",
            "Epoch: 0087 cost= 0.000904682 lr =  0.0022588600\n",
            "Epoch: 0088 cost= 0.000899178 lr =  0.0022207657\n",
            "Epoch: 0089 cost= 0.000893792 lr =  0.0021833135\n",
            "Epoch: 0090 cost= 0.000888529 lr =  0.0021443467\n",
            "Epoch: 0091 cost= 0.000883380 lr =  0.0021081835\n",
            "Epoch: 0092 cost= 0.000878336 lr =  0.0020726300\n",
            "Epoch: 0093 cost= 0.000873419 lr =  0.0020376761\n",
            "Epoch: 0094 cost= 0.000868603 lr =  0.0020033119\n",
            "Epoch: 0095 cost= 0.000863907 lr =  0.0019695272\n",
            "Epoch: 0096 cost= 0.000859320 lr =  0.0019363119\n",
            "Epoch: 0097 cost= 0.000854836 lr =  0.0019036572\n",
            "Epoch: 0098 cost= 0.000850467 lr =  0.0018715530\n",
            "Epoch: 0099 cost= 0.000846201 lr =  0.0018399904\n",
            "Epoch: 0100 cost= 0.000842028 lr =  0.0018071508\n",
            "Optimization Finished!\n",
            "Train Error: 0.0008251991\n",
            "Test Error: 0.0009180632\n",
            "Test MSE: 4990.1523\n",
            "Test MAPE: 0.09930442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw49GOpORTqm",
        "colab_type": "code",
        "outputId": "1217f93f-156e-461f-9dd3-94222236f535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        print(\"Train Error before LM:\", acc.eval({x: x_train_ori, y: y_train0}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Error before LM: 0.0008251991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FrIJ-IAfF9A",
        "colab_type": "code",
        "outputId": "507cf74a-ad1e-428b-e40c-ee798d5e5023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "feed_dict = {x: x_train_ori,\n",
        "             y: y_train0}\n",
        "feed_dict[mu] = init_mu\n",
        "\n",
        "# construct so-called feed dictionary to map placeholders to actual values\n",
        "\n",
        "validation_feed_dict = {x: x_test_ori,\n",
        "                        y: y_test0}\n",
        "\n",
        "train_break = False\n",
        "\n",
        "current_loss = sess.run(cost,{x: x_train_ori,y: y_train0})\n",
        "\n",
        "\n",
        "# Start training\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        epoch = 1\n",
        "        fail_step = 0\n",
        "        # Training cycle\n",
        "        while epoch < epochs and current_loss > target_mse:\n",
        "            if not(epoch%2) or epoch == 1:\n",
        "                val_loss = sess.run(cost, validation_feed_dict)\n",
        "                print('epoch: %3d , mu: %.5e, train loss: %.10f, val loss: %.10f'%(epoch,feed_dict[mu],current_loss,val_loss))\n",
        "            sess.run(save_parms)\n",
        "            while True:\n",
        "                start_step = time.time()\n",
        "                sess.run(lm, feed_dict)\n",
        "                new_loss = sess.run(cost,feed_dict)\n",
        "                # print('One update ended in %d seconds' % (time.time()-start_step))\n",
        "                if new_loss > current_loss:\n",
        "                    fail_step += 1\n",
        "                    feed_dict[mu] *= 10\n",
        "                    if feed_dict[mu] > mu_max or fail_step > max_fail:\n",
        "                        train_break = True\n",
        "                        break\n",
        "                    sess.run(restore_parms)\n",
        "                else:\n",
        "                    print(\"mu: %.5e, new loss: %.5f, previous loss: %.5f\"%(feed_dict[mu],new_loss,current_loss))\n",
        "                    fail_step = 0\n",
        "                    feed_dict[mu] /= 10\n",
        "                    feed_dict[mu] = max(1e-20,feed_dict[mu])\n",
        "                    current_loss = new_loss\n",
        "                    break\n",
        "            if train_break:\n",
        "                print('Failed for %d step(s), current mu = %.5e, stop training' % (fail_step,feed_dict[mu]))\n",
        "                break\n",
        "            epoch += 1\n",
        "    print(\"Location #%d Optimization Finished in %d seconds!\" %(train_loc, time.time() - start))\n",
        "\n",
        "    # Test model\n",
        "\n",
        "    y_train_predicted = sess.run(pred,feed_dict)\n",
        "    y_test_predicted = sess.run(pred,validation_feed_dict)\n",
        "\n",
        "    show_eval(y_train_predicted[:,0],\n",
        "              y_train0[:,0],\n",
        "              y_test_predicted[:,0],\n",
        "              y_test0[:,0],\n",
        "              y_slope,y_bias,ann_name)\n",
        "    writeF90('fnet_'+abbrev_map[output_stations[0].lower()]+\".f90\",\n",
        "             y_slope[0],y_bias[0],\n",
        "             sess.run(W1).transpose(),sess.run(b1),\n",
        "             sess.run(W2).transpose(),sess.run(b2),\n",
        "             sess.run(W3).transpose(),sess.run(b3))\n",
        "\n",
        "model_path = \"./%s/model.ckpt\" % (abbrev_map[output_stations[0].lower()])\n",
        "save_path = saver.save(sess, model_path)\n",
        "print(\"Model saved in path: %s\" % save_path)\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        saved_test_loss = acc.eval(validation_feed_dict)\n",
        "        print(\"Train Error after LM:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "        print(\"Test Error after LM:\", saved_test_loss)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:   1 , mu: 5.00000e-02, train loss: 0.0008251991, val loss: 0.0009180632\n",
            "mu: 5.00000e+00, new loss: 0.00077, previous loss: 0.00083\n",
            "epoch:   2 , mu: 5.00000e-01, train loss: 0.0007654151, val loss: 0.0008555299\n",
            "mu: 5.00000e+00, new loss: 0.00076, previous loss: 0.00077\n",
            "mu: 5.00000e+00, new loss: 0.00076, previous loss: 0.00076\n",
            "epoch:   4 , mu: 5.00000e-01, train loss: 0.0007552777, val loss: 0.0008426187\n",
            "mu: 5.00000e+00, new loss: 0.00075, previous loss: 0.00076\n",
            "mu: 5.00000e+00, new loss: 0.00075, previous loss: 0.00075\n",
            "epoch:   6 , mu: 5.00000e-01, train loss: 0.0007471853, val loss: 0.0008325002\n",
            "mu: 5.00000e+00, new loss: 0.00074, previous loss: 0.00075\n",
            "mu: 5.00000e-01, new loss: 0.00074, previous loss: 0.00074\n",
            "epoch:   8 , mu: 5.00000e-02, train loss: 0.0007387704, val loss: 0.0008195420\n",
            "mu: 5.00000e-01, new loss: 0.00069, previous loss: 0.00074\n",
            "mu: 5.00000e-01, new loss: 0.00067, previous loss: 0.00069\n",
            "epoch:  10 , mu: 5.00000e-02, train loss: 0.0006720041, val loss: 0.0007440913\n",
            "mu: 5.00000e-01, new loss: 0.00065, previous loss: 0.00067\n",
            "mu: 5.00000e-02, new loss: 0.00064, previous loss: 0.00065\n",
            "epoch:  12 , mu: 5.00000e-03, train loss: 0.0006417679, val loss: 0.0007093871\n",
            "mu: 5.00000e-03, new loss: 0.00052, previous loss: 0.00064\n",
            "mu: 5.00000e-03, new loss: 0.00044, previous loss: 0.00052\n",
            "epoch:  14 , mu: 5.00000e-04, train loss: 0.0004358012, val loss: 0.0004808996\n",
            "mu: 5.00000e-02, new loss: 0.00040, previous loss: 0.00044\n",
            "mu: 5.00000e-03, new loss: 0.00039, previous loss: 0.00040\n",
            "epoch:  16 , mu: 5.00000e-04, train loss: 0.0003903598, val loss: 0.0004407380\n",
            "mu: 5.00000e-02, new loss: 0.00038, previous loss: 0.00039\n",
            "mu: 5.00000e-02, new loss: 0.00037, previous loss: 0.00038\n",
            "epoch:  18 , mu: 5.00000e-03, train loss: 0.0003711379, val loss: 0.0004191490\n",
            "mu: 5.00000e-02, new loss: 0.00037, previous loss: 0.00037\n",
            "mu: 5.00000e-02, new loss: 0.00037, previous loss: 0.00037\n",
            "epoch:  20 , mu: 5.00000e-03, train loss: 0.0003666165, val loss: 0.0004150712\n",
            "mu: 5.00000e-02, new loss: 0.00036, previous loss: 0.00037\n",
            "mu: 5.00000e-02, new loss: 0.00036, previous loss: 0.00036\n",
            "epoch:  22 , mu: 5.00000e-03, train loss: 0.0003627043, val loss: 0.0004117688\n",
            "mu: 5.00000e-02, new loss: 0.00036, previous loss: 0.00036\n",
            "mu: 5.00000e-02, new loss: 0.00036, previous loss: 0.00036\n",
            "epoch:  24 , mu: 5.00000e-03, train loss: 0.0003592959, val loss: 0.0004091855\n",
            "mu: 5.00000e-02, new loss: 0.00036, previous loss: 0.00036\n",
            "mu: 5.00000e-02, new loss: 0.00036, previous loss: 0.00036\n",
            "epoch:  26 , mu: 5.00000e-03, train loss: 0.0003563438, val loss: 0.0004072044\n",
            "mu: 5.00000e-02, new loss: 0.00036, previous loss: 0.00036\n",
            "mu: 5.00000e-02, new loss: 0.00035, previous loss: 0.00036\n",
            "epoch:  28 , mu: 5.00000e-03, train loss: 0.0003537889, val loss: 0.0004056968\n",
            "mu: 5.00000e-02, new loss: 0.00035, previous loss: 0.00035\n",
            "mu: 5.00000e-02, new loss: 0.00035, previous loss: 0.00035\n",
            "epoch:  30 , mu: 5.00000e-03, train loss: 0.0003515997, val loss: 0.0004045882\n",
            "mu: 5.00000e-02, new loss: 0.00035, previous loss: 0.00035\n",
            "mu: 5.00000e-03, new loss: 0.00035, previous loss: 0.00035\n",
            "epoch:  32 , mu: 5.00000e-04, train loss: 0.0003496431, val loss: 0.0004072487\n",
            "mu: 5.00000e-03, new loss: 0.00034, previous loss: 0.00035\n",
            "mu: 5.00000e-03, new loss: 0.00034, previous loss: 0.00034\n",
            "epoch:  34 , mu: 5.00000e-04, train loss: 0.0003388890, val loss: 0.0004018722\n",
            "mu: 5.00000e-02, new loss: 0.00033, previous loss: 0.00034\n",
            "mu: 5.00000e-03, new loss: 0.00033, previous loss: 0.00033\n",
            "epoch:  36 , mu: 5.00000e-04, train loss: 0.0003325194, val loss: 0.0003964466\n",
            "mu: 5.00000e-03, new loss: 0.00033, previous loss: 0.00033\n",
            "mu: 5.00000e-03, new loss: 0.00033, previous loss: 0.00033\n",
            "epoch:  38 , mu: 5.00000e-04, train loss: 0.0003267580, val loss: 0.0003926140\n",
            "mu: 5.00000e-03, new loss: 0.00032, previous loss: 0.00033\n",
            "mu: 5.00000e-03, new loss: 0.00032, previous loss: 0.00032\n",
            "epoch:  40 , mu: 5.00000e-04, train loss: 0.0003233911, val loss: 0.0003904094\n",
            "mu: 5.00000e-03, new loss: 0.00032, previous loss: 0.00032\n",
            "mu: 5.00000e-04, new loss: 0.00032, previous loss: 0.00032\n",
            "epoch:  42 , mu: 5.00000e-05, train loss: 0.0003213184, val loss: 0.0003879062\n",
            "mu: 5.00000e-03, new loss: 0.00032, previous loss: 0.00032\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00032\n",
            "epoch:  44 , mu: 5.00000e-04, train loss: 0.0003149523, val loss: 0.0003849957\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "mu: 5.00000e-04, new loss: 0.00031, previous loss: 0.00031\n",
            "epoch:  46 , mu: 5.00000e-05, train loss: 0.0003139450, val loss: 0.0003836352\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "epoch:  48 , mu: 5.00000e-04, train loss: 0.0003098044, val loss: 0.0003798815\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "epoch:  50 , mu: 5.00000e-04, train loss: 0.0003085191, val loss: 0.0003779936\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "mu: 5.00000e-02, new loss: 0.00031, previous loss: 0.00031\n",
            "epoch:  52 , mu: 5.00000e-03, train loss: 0.0003077269, val loss: 0.0003764949\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "epoch:  54 , mu: 5.00000e-04, train loss: 0.0003065919, val loss: 0.0003744582\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00031\n",
            "epoch:  56 , mu: 5.00000e-04, train loss: 0.0003054426, val loss: 0.0003724654\n",
            "mu: 5.00000e-02, new loss: 0.00031, previous loss: 0.00031\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00031\n",
            "epoch:  58 , mu: 5.00000e-04, train loss: 0.0003047183, val loss: 0.0003709235\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "epoch:  60 , mu: 5.00000e-04, train loss: 0.0003033229, val loss: 0.0003684309\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "epoch:  62 , mu: 5.00000e-04, train loss: 0.0003018224, val loss: 0.0003658567\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "epoch:  64 , mu: 5.00000e-04, train loss: 0.0003004027, val loss: 0.0003636176\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "epoch:  66 , mu: 5.00000e-04, train loss: 0.0002988612, val loss: 0.0003611035\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "epoch:  68 , mu: 5.00000e-04, train loss: 0.0002975022, val loss: 0.0003589491\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00030\n",
            "mu: 5.00000e-04, new loss: 0.00030, previous loss: 0.00030\n",
            "epoch:  70 , mu: 5.00000e-05, train loss: 0.0002957246, val loss: 0.0003561843\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00030\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  72 , mu: 5.00000e-03, train loss: 0.0002938530, val loss: 0.0003543215\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  74 , mu: 5.00000e-04, train loss: 0.0002933858, val loss: 0.0003532594\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  76 , mu: 5.00000e-03, train loss: 0.0002929356, val loss: 0.0003525052\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  78 , mu: 5.00000e-04, train loss: 0.0002923990, val loss: 0.0003516746\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-04, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  80 , mu: 5.00000e-05, train loss: 0.0002915747, val loss: 0.0003504241\n",
            "mu: 5.00000e-04, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  82 , mu: 5.00000e-03, train loss: 0.0002904577, val loss: 0.0003494261\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  84 , mu: 5.00000e-03, train loss: 0.0002903152, val loss: 0.0003491008\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  86 , mu: 5.00000e-03, train loss: 0.0002902050, val loss: 0.0003488617\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  88 , mu: 5.00000e-04, train loss: 0.0002900137, val loss: 0.0003480172\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  90 , mu: 5.00000e-03, train loss: 0.0002898885, val loss: 0.0003475956\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  92 , mu: 5.00000e-04, train loss: 0.0002897359, val loss: 0.0003472076\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  94 , mu: 5.00000e-03, train loss: 0.0002896079, val loss: 0.0003467767\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  96 , mu: 5.00000e-04, train loss: 0.0002895003, val loss: 0.0003463452\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "epoch:  98 , mu: 5.00000e-04, train loss: 0.0002894777, val loss: 0.0003458561\n",
            "mu: 5.00000e-02, new loss: 0.00029, previous loss: 0.00029\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00029\n",
            "Location #3 Optimization Finished in 2100 seconds!\n",
            "train mape:  0.0649\n",
            "train mse:  1572.69\n",
            "test mape:  0.0668\n",
            "test mse: 1877.85\n",
            "<class 'numpy.float32'>\n",
            "Model saved in path: ./ORRSL/model.ckpt\n",
            "Train Error after LM: 0.00028933515\n",
            "Test Error after LM: 0.0003454774\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH8xqOweXNJ6HAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO3TvhKyz30q",
        "colab_type": "code",
        "outputId": "9bcfa9a6-f064-41cc-e815-024b1e6a1dec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "## download fortran file and trained model\n",
        "model_dir = abbrev_map[output_stations[0].lower()]\n",
        "from google.colab import files\n",
        "ann_zip = ann_name +'.zip'\n",
        "f90name = 'fnet_'+ann_name+'.f90'\n",
        "!zip -r /content/$ann_zip $model_dir\n",
        "!zip -r /content/$ann_zip $f90name"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: ORRSL/ (stored 0%)\n",
            "updating: ORRSL/model.ckpt.meta (deflated 86%)\n",
            "updating: ORRSL/model.ckpt.index (deflated 32%)\n",
            "updating: ORRSL/checkpoint (deflated 42%)\n",
            "updating: ORRSL/model.ckpt.data-00000-of-00001 (deflated 5%)\n",
            "  adding: fnet_ORRSL.f90 (deflated 56%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN1miFnUkHiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('/content/%s' % ann_zip)\n",
        "# files.download('%s.jpg'%(output_stations[0]+str(fig_index)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}