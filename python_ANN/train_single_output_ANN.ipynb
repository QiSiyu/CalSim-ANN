{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "train_single_output_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbLsjPUk3oIb"
      },
      "source": [
        "## Train an STL ANN from scratch\n",
        "# User settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHPnGTMf95oI"
      },
      "source": [
        "######## User Settings ########\n",
        "\n",
        "#### 1. Select one or more input variables from:\n",
        "# 'SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC'\n",
        "\n",
        "input_var = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
        "\n",
        "# 2. Select ONE output station among:\n",
        "# 'Emmaton', 'Jersey Point', 'Collinsville', 'Rock Slough', 'Antioch', 'Mallard',\n",
        "# 'LosVaqueros', 'Martinez', 'MiddleRiver', 'Vict Intake', 'CVP Intake', 'CCFB_OldR'\n",
        "\n",
        "output_stations=['CVP Intake']\n",
        "\n",
        "# 3. Specify directory to excel dataset and the helper script (folder name only) ####\n",
        "google_drive_dir = 'python_ANN'\n",
        "\n",
        "# 4. Specify whether:\n",
        "#     -- to train the ANN(s) ==> set to 'no'\n",
        "#     -- to quickly test the code, in which case ANN won't be well-trained ==> set to 'yes'\n",
        "is_quick_test = 'yes'\n",
        "\n",
        "# 5. Specify whether running the code on Colab or a local computer\n",
        "# NOTE: if set to False, please set option #3: google_drive_dir to the local path of python_ANN\n",
        "#       e.g., google_drive_dir='/Users/siyuqi/Downloads/Calsim-ANN-master/python_ANN'\n",
        "running_on_colab = True\n",
        "\n",
        "###### User Settings Finished ######\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Wd7z3l996j"
      },
      "source": [
        "# Mount Google Drive to Colab and import useful libraries\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6GxpZSUj0dF"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Mount Google drive\n",
        "if running_on_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    xl_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"ANN_data.xlsx\")\n",
        "    %tensorflow_version 1.x\n",
        "    sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir))\n",
        "else:\n",
        "    xl_path = os.path.join(os.path.dirname(google_drive_dir),\"ANN_data.xlsx\")\n",
        "    sys.path.append(os.path.join(google_drive_dir))\n",
        "\n",
        "# import helper functions\n",
        "from ann_helper import read_data,normalize_in,writeF90,initnw,show_eval\n",
        "\n",
        "\n",
        "# determine whether to run a quick test or a full training\n",
        "if 'n' in is_quick_test.lower():\n",
        "    test_mode = False\n",
        "else:\n",
        "    test_mode = True\n",
        "    print('Running a quick test...')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmq34EstqnG8"
      },
      "source": [
        "# Prepare for training\n",
        "\n",
        "\n",
        "1.   Define hyper-parameters\n",
        "2.   Read data from excel\n",
        "3.   Split train/test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6348s2lzcfC"
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "from scipy import stats\n",
        "\n",
        "# sort input and output variables\n",
        "locs = {'Emmaton':0,'Jersey Point':1,'Collinsville':2,'Rock Slough':3,'Antioch':4,\n",
        "        'Mallard':5, 'LosVaqueros':6, 'Martinez':7, 'MiddleRiver':8, 'Vict Intake':9,\n",
        "        'CVP Intake':10, 'CCFB_OldR':11}\n",
        "abbrev_map = {'rock slough':'ORRSL','rockslough':'ORRSL',\n",
        "            'emmaton':'EMM','jersey point':'JP','jerseypoint':'JP',\n",
        "            'antioch':'antioch','collinsville':'CO',\n",
        "            'mallard':'Mallard','mallard island':'Mallard',\n",
        "            'los vaqueros':'LosVaqueros','losvaqueros':'LosVaqueros',\n",
        "            'martinez':'MTZ',\n",
        "            'middle river':'MidR_intake','middleriver':'MidR_intake',\n",
        "            'victoria cannal':'Victoria_intake','vict intake':'Victoria_intake',\n",
        "            'cvp intake':'CVP_intake','clfct forebay':'CCFB',\n",
        "            'clfct forebay intake':'ccfb_intake','x2':'X2'};\n",
        "\n",
        "output_stations = sorted(output_stations,key=lambda x: locs[x])\n",
        "\n",
        "\n",
        "# detect available device: CPU or GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if 'gpu' not in device_name.lower():\n",
        "    print('Found CPU only')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "# determine hyper-parameters\n",
        "input_shape = (1,18*len(input_var))\n",
        "output_shape = 1\n",
        "nn_shape = [18*len(input_var),8,2,1] # number of neurons in each layer\n",
        "\n",
        "# LM optimizer settings\n",
        "max_fail = 3\n",
        "init_mu = .05\n",
        "mu_max = 1e10\n",
        "target_mse = 0.\n",
        "\n",
        "# Adam optimizer settings\n",
        "adam_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "if test_mode or (device_name != '/device:GPU:0' and running_on_colab):\n",
        "    epochs = 5\n",
        "else:\n",
        "    epochs = 100\n",
        "\n",
        "train_loc = locs[output_stations[0]]\n",
        "ann_name = abbrev_map[output_stations[0].lower()]\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "# read data from excel\n",
        "x_data,y_data = read_data(xl_path,input_var,output_stations)\n",
        "end = time.time()\n",
        "print(\"loading data in %.2f seconds\" % (end-start) )\n",
        "\n",
        "\n",
        "# normalize data to 0.1 ~ 0.9\n",
        "[x_norm,x_slope,x_bias] = normalize_in(x_data)\n",
        "[y_norm,y_slope,y_bias] = normalize_in(y_data)\n",
        "\n",
        "\n",
        "# split 80% for training, 20% for testing\n",
        "x_train_ori, x_test_ori, y_train0, y_test0 = train_test_split(x_norm,\n",
        "                                                              y_norm,\n",
        "                                                              test_size=0.2,\n",
        "                                                              random_state = 0)\n",
        "\n",
        "if test_mode:\n",
        "    x_train_ori = x_train_ori[:100]\n",
        "    x_test_ori = x_test_ori[:100]\n",
        "    y_train0 = y_train0[:100]\n",
        "    y_test0 = y_test0[:100]\n",
        "\n",
        "\n",
        "train_err = []\n",
        "test_err = []\n",
        "train_shape = len(x_train_ori)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d9VLRePrmy_"
      },
      "source": [
        "# Prepare the ANN\n",
        "\n",
        "\n",
        "1.   Build and connect ANN layers\n",
        "2.   Define cost function\n",
        "3.   Create an Adam optimizer object\n",
        "4.   Create an LM optimizer object and implement the LM algorithm \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZNeDaHG0fyp"
      },
      "source": [
        "# Define inputs, outputs and weights for the ANN, and build ANN using these variables\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, 18*len(input_var)], name='InputData')\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, 1], name='LabelData')\n",
        "\n",
        "init_val = initnw(list(zip(*(nn_shape[i:] for i in range(2)))),x_train_ori)\n",
        "\n",
        "W1 = tf.Variable(initial_value=init_val[0][0], name='w1',dtype='float32')\n",
        "b1 = tf.Variable(initial_value=init_val[0][1], name='b1',dtype='float32')\n",
        "W2 = tf.Variable(initial_value=init_val[1][0], name='w2',dtype='float32')\n",
        "b2 = tf.Variable(initial_value=init_val[1][1], name='b2',dtype='float32')\n",
        "W3 = tf.Variable(initial_value=init_val[2][0], name='w3',dtype='float32')\n",
        "b3 = tf.Variable(initial_value=init_val[2][1], name='b3',dtype='float32')\n",
        "\n",
        "with tf.compat.v1.name_scope('layer1'):\n",
        "    first_out = tf.sigmoid(tf.add(tf.matmul(x,W1),b1))\n",
        "with tf.compat.v1.name_scope('layer2'):\n",
        "    second_out = tf.sigmoid(tf.add(tf.matmul(first_out,W2),b2))\n",
        "with tf.compat.v1.name_scope('layer3'):\n",
        "    pred = tf.matmul(second_out, W3) + b3\n",
        "\n",
        "# r is residuals between target output and ANN estimations\n",
        "r = tf.subtract(pred,y)\n",
        "\n",
        "# Cost/loss function, which must be mse or sse for LM optimizer\n",
        "cost = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "\n",
        "# Define useful variables for adam optimizer:\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "initial_lr = 1e-2\n",
        "learning_rate = tf.compat.v1.train.exponential_decay(initial_lr, global_step,\n",
        "                                       40, 0.999, staircase=True)\n",
        "# Build Adam optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    adam_opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step = global_step)\n",
        "\n",
        "# Build LM optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=1)\n",
        "\n",
        "with tf.compat.v1.name_scope('Accuracy'):\n",
        "    acc = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "# Initialize the variables\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "\n",
        "# Implement LM algorithm\n",
        "def jacobian(y, x):\n",
        "    loop_vars = [\n",
        "        tf.constant(0, tf.int32),\n",
        "        tf.TensorArray(tf.float32, size=train_shape),\n",
        "    ]\n",
        "\n",
        "    _, jacobian = tf.while_loop(\n",
        "        cond=lambda i, _: i < train_shape,\n",
        "        body=lambda i, res: (i+1, res.write(i, tf.reshape(tf.gradients(ys=y[i], xs=x), (-1,)))),\n",
        "        loop_vars=loop_vars)\n",
        "    return jacobian.stack()\n",
        "\n",
        "\n",
        "r_flat = tf.expand_dims(tf.reshape(r,[-1]),1)\n",
        "parms = [W1, b1, W2, b2, W3, b3]\n",
        "parms_sizes = [tf.size(input=p) for p in parms]\n",
        "j = tf.concat([jacobian(r_flat, p) for p in parms], 1)\n",
        "jT = tf.transpose(a=j)\n",
        "hess_approx = tf.matmul(jT, j)\n",
        "grad_approx = tf.matmul(jT, r_flat)\n",
        "\n",
        "mu = tf.compat.v1.placeholder(tf.float32, shape=[])\n",
        "\n",
        "store = [tf.Variable(tf.zeros(p.shape, dtype=tf.float32)) for p in parms]\n",
        "save_parms = [tf.compat.v1.assign(s, p) for s, p in zip(store, parms)]\n",
        "restore_parms = [tf.compat.v1.assign(p, s) for s, p in zip(store, parms)]\n",
        "\n",
        "wb_flat = tf.concat([tf.reshape(p,[-1,1]) for p in parms],axis=0)\n",
        "n = tf.add_n(parms_sizes)\n",
        "I = tf.eye(n, dtype=tf.float32)\n",
        "w_2 = tf.reduce_sum(input_tensor=tf.square(wb_flat))\n",
        "\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(init)\n",
        "\n",
        "dp_flat = tf.matmul(tf.linalg.inv(hess_approx + tf.multiply(mu, I)), grad_approx)\n",
        "dps = tf.split(dp_flat, parms_sizes, 0)\n",
        "\n",
        "for i in range(len(dps)):\n",
        "    dps[i] = tf.reshape(dps[i], parms[i].shape)\n",
        "lm = opt.apply_gradients(zip(dps, parms))\n",
        "\n",
        "\n",
        "# print number of parameters\n",
        "print(\"Number of trainable parameters: %d\" % sess.run(n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yl7JfhOtpyt"
      },
      "source": [
        "Create a saver object to save trained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgyRTmYsLR4"
      },
      "source": [
        "saver = tf.compat.v1.train.Saver({'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqI3OZs-tsJ6"
      },
      "source": [
        "# Train the ANN with Adam optimizer \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy1XKPUeKd10"
      },
      "source": [
        "# Function that feeds data batches to the ANN\n",
        "def get_batch(inputX, inputY, batch_size):\n",
        "    duration = len(inputX)\n",
        "    for i in range(0,duration//batch_size):\n",
        "        idx = i*batch_size\n",
        "        yield inputX[idx:idx+batch_size], inputY[idx:idx+batch_size]\n",
        "\n",
        "\n",
        "\n",
        "# Start training\n",
        "\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        cost_list = []\n",
        "        lr_list = []\n",
        "        # Run the initializer\n",
        "        sess.run(init)\n",
        "\n",
        "        # Training cycle\n",
        "        total_batch = train_shape//batch_size\n",
        "        for epoch in range(adam_epochs):\n",
        "            avg_cost = 0.\n",
        "            ii = 0\n",
        "            # Loop over all batches\n",
        "            for batch_xs, batch_ys in get_batch(x_train_ori, y_train0,batch_size):\n",
        "                ii = ii +1\n",
        "                # Run optimization op (backprop), cost op (to get loss value)\n",
        "                # and summary nodes\n",
        "                _, c = sess.run([adam_opt, cost],\n",
        "                                 feed_dict={x: batch_xs, y: batch_ys})\n",
        "                # Write logs at every iteration\n",
        "                # Compute average loss\n",
        "                avg_cost += c / total_batch\n",
        "            lr = learning_rate.eval()\n",
        "            cost_list.append(avg_cost)\n",
        "            lr_list.append(lr)\n",
        "            # Display logs per epoch step\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost),\n",
        "                  \"lr = \",\"{:.10f}\".format(lr))\n",
        "\n",
        "    print(\"Adam optimization Finished in %d seconds\" %(time.time() - start))\n",
        "\n",
        "    # Evaluate model performance\n",
        "    print(\"Train MSE:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "    print(\"Test MSE:\", acc.eval({x: x_test_ori, y: y_test0}))\n",
        "    y_test_ = sess.run(pred,feed_dict={x:x_test_ori})\n",
        "    print(\"Test MAPE:\", np.mean(np.abs(y_test_-y_test0)/y_test0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VroAu6AuhCl"
      },
      "source": [
        "# Fine-tune the ANN with LM optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FrIJ-IAfF9A"
      },
      "source": [
        "# Define input and output datasets for training\n",
        "feed_dict = {x: x_train_ori,\n",
        "             y: y_train0}\n",
        "feed_dict[mu] = init_mu\n",
        "\n",
        "# Define input and output datasets for validation\n",
        "validation_feed_dict = {x: x_test_ori,\n",
        "                        y: y_test0}\n",
        "\n",
        "train_break = False\n",
        "current_loss = sess.run(cost,{x: x_train_ori,y: y_train0})\n",
        "\n",
        "# Prepare saving directory\n",
        "if running_on_colab and (not os.path.exists(os.path.join('/content/drive', 'My Drive', google_drive_dir,\"models\",ann_name))):\n",
        "    os.makedirs(os.path.join('/content/drive',\n",
        "                             'My Drive',\n",
        "                             google_drive_dir,\n",
        "                             \"models\",\n",
        "                             ann_name))\n",
        "\n",
        "\n",
        "# Start training\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        epoch = 1\n",
        "        fail_step = 0\n",
        "        # Training cycle\n",
        "        while epoch < epochs and current_loss > target_mse:\n",
        "            epoch_start = time.time()\n",
        "            if not(epoch%2) or epoch == 1:\n",
        "                val_loss = sess.run(cost, validation_feed_dict)\n",
        "                print('epoch: %3d , mu: %.5e, train loss: %.10f, val loss: %.10f'%(epoch,feed_dict[mu],current_loss,val_loss))\n",
        "            sess.run(save_parms)\n",
        "            while True:\n",
        "                sess.run(lm, feed_dict)\n",
        "                new_loss = sess.run(cost,feed_dict)\n",
        "                if new_loss > current_loss:\n",
        "                    fail_step += 1\n",
        "                    feed_dict[mu] *= 10\n",
        "                    if feed_dict[mu] > mu_max or fail_step > max_fail:\n",
        "                        train_break = True\n",
        "                        break\n",
        "                    sess.run(restore_parms)\n",
        "                else:\n",
        "                    print(\"mu: %.5e, new loss: %.5f, previous loss: %.5f\"%(feed_dict[mu],new_loss,current_loss))\n",
        "                    fail_step = 0\n",
        "                    feed_dict[mu] /= 10\n",
        "                    feed_dict[mu] = max(1e-20,feed_dict[mu])\n",
        "                    current_loss = new_loss\n",
        "                    break\n",
        "            if train_break:\n",
        "                print('Failed for %d step(s), current mu = %.5e, stop training' % (fail_step,feed_dict[mu]))\n",
        "                break\n",
        "            epoch += 1\n",
        "            print('Time elapsed: %.1f seconds\\n'%(time.time() - epoch_start))\n",
        "    print(\"Optimization finished in %d seconds!\\n\" %(time.time() - start))\n",
        "\n",
        "    # Evaluate model performance\n",
        "    y_train_predicted = sess.run(pred,feed_dict)\n",
        "    y_test_predicted = sess.run(pred,validation_feed_dict)\n",
        "\n",
        "\n",
        "# Save model to current workspace\n",
        "model_path = \"./%s/model.ckpt\" % (ann_name)\n",
        "save_path = saver.save(sess, model_path)\n",
        "print(\"Model saved to current workspace: %s\" % os.path.abspath(save_path))\n",
        "\n",
        "# Save model to Google Drive\n",
        "if running_on_colab:\n",
        "    model_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"models/%s/model.ckpt\"%ann_name)\n",
        "    save_path = saver.save(sess, model_path)\n",
        "    print(\"Model saved to Google Drive: %s\" % os.path.abspath(save_path))\n",
        "\n",
        "# Visualize results\n",
        "show_eval(y_train_predicted[:,0],\n",
        "          y_train0[:,0],\n",
        "          y_test_predicted[:,0],\n",
        "          y_test0[:,0],\n",
        "          y_slope,y_bias,ann_name)\n",
        "\n",
        "# Write weights to fortran file\n",
        "writeF90(ann_name,abbrev_map[output_stations[0].lower()],\n",
        "          y_slope[0],y_bias[0],\n",
        "          sess.run(W1).transpose(),sess.run(b1),\n",
        "          sess.run(W2).transpose(),sess.run(b2),\n",
        "          sess.run(W3).transpose(),sess.run(b3))\n",
        "\n",
        "# Write weights to fortran file to Google Drive\n",
        "if running_on_colab:\n",
        "    writeF90(os.path.join('/content/drive','My Drive',google_drive_dir,\"models/%s\"%ann_name),abbrev_map[output_stations[0].lower()],\n",
        "            y_slope[0],y_bias[0],\n",
        "            sess.run(W1).transpose(),sess.run(b1),\n",
        "            sess.run(W2).transpose(),sess.run(b2),\n",
        "            sess.run(W3).transpose(),sess.run(b3))\n",
        "\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        saved_test_loss = acc.eval(validation_feed_dict)\n",
        "        print(\"Train Error after LM:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "        print(\"Test Error after LM:\", saved_test_loss)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mtf5Sbw_0Ubd"
      },
      "source": [
        "# Save true output data and ANN estimations into csv to current working directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_AG1MkXT3Uc"
      },
      "source": [
        "import pandas as pd\n",
        "input_data =  x_norm\n",
        "output_data = y_norm\n",
        "feed_dict = {x: input_data,y: output_data}\n",
        "\n",
        "loc = output_stations[0]\n",
        "results_dir = \"./%s\" % (ann_name)\n",
        "google_drive_results_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"models/%s\"%ann_name)\n",
        "date = np.arange('1940-10',\n",
        "                 np.datetime64('1940-10') + np.timedelta64(len(input_data), 'D'),\n",
        "                 dtype='datetime64[D]')\n",
        "\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        # let ANN compute estimations\n",
        "        y_predicted = sess.run(pred,feed_dict)\n",
        "        MSE = np.mean(((y_predicted-output_data)/y_slope[0])**2,axis=0)\n",
        "        MAPE = np.mean(np.abs(y_predicted-output_data)/output_data,axis=0)\n",
        "        # print estimation errors\n",
        "        print(\"MSE = %d\" % MSE)\n",
        "        print(\"MAPE = %.2f%%\" % (MAPE*100))\n",
        "\n",
        "# write ANN estimations to csv\n",
        "results = pd.DataFrame(data=((y_predicted-y_bias[0])/y_slope[0]).reshape(-1,),\n",
        "                        index=date,\n",
        "                        columns=[loc])\n",
        "results.index.name='date    '\n",
        "results.to_csv(os.path.join(results_dir,'%s_ANN_estimations.txt'%ann_name),\n",
        "                sep='\\t',\n",
        "                float_format='%5.4f',\n",
        "                header=True,\n",
        "                index=True)\n",
        "# print paths of the csv files\n",
        "print('-'*65)\n",
        "print('-'*65)\n",
        "print(\"ANN estimations written to: \\n%s\" % (os.path.abspath(os.path.join(results_dir,'%s_ANN_estimations.txt'%ann_name))))\n",
        "if running_on_colab:\n",
        "    results.to_csv(os.path.join(google_drive_results_path,'%s_ANN_estimations.txt'%ann_name),\n",
        "                    sep='\\t',\n",
        "                    float_format='%5.4f',\n",
        "                    header=True,\n",
        "                    index=True)\n",
        "    print(os.path.join(google_drive_results_path,'%s_ANN_estimations.txt'%ann_name))\n",
        "\n",
        "print('-'*65)\n",
        "\n",
        "# write target output values to csv\n",
        "real_data = pd.DataFrame(data=(output_data/y_slope[0]).reshape(-1,),\n",
        "                        index=date,\n",
        "                        columns=[loc])\n",
        "real_data.index.name='date    '\n",
        "real_data.to_csv(os.path.join(results_dir,'%s_target_outputs.txt'%ann_name),\n",
        "                  sep='\\t',\n",
        "                  float_format='%5.4f',\n",
        "                  header=True,\n",
        "                  index=True)\n",
        "\n",
        "# print paths of the csv files\n",
        "print(\"True salinity values written to: \\n%s\" %(os.path.abspath(os.path.join(results_dir,'%s_target_outputs.txt'%ann_name))))\n",
        "\n",
        "# Also save csv files to Google Drive\n",
        "if running_on_colab:\n",
        "    real_data.to_csv(os.path.join(google_drive_results_path,'%s_target_outputs.txt'%ann_name),\n",
        "                      sep='\\t',\n",
        "                      float_format='%5.4f',\n",
        "                      header=True,\n",
        "                      index=True)\n",
        "    print(os.path.join(google_drive_results_path,'%s_target_outputs.txt'%ann_name))\n",
        "\n",
        "print('-'*65)\n",
        "print('-'*65)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cho9bDf2czQS"
      },
      "source": [
        "# Compress trained model and fortran file (on Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO3TvhKyz30q"
      },
      "source": [
        "if running_on_colab:\n",
        "    from google.colab import files\n",
        "    ann_zip = ann_name +'.zip'\n",
        "    !zip -r /content/$ann_zip $ann_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93WFD4Wqcw7q"
      },
      "source": [
        "# Download zip file (from Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN1miFnUkHiE"
      },
      "source": [
        "if running_on_colab:\n",
        "    files.download('/content/%s' % ann_zip)\n",
        "    # files.download('%s.jpg'%(output_stations[0]+str(fig_index)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}