{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_single_output_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbLsjPUk3oIb",
        "colab_type": "text"
      },
      "source": [
        "## Training a Single-Output ANN from scratch\n",
        "\n",
        "Note: this script trains only one ANN for the station selected by user. Please set 'output_stations' to different stations for training multiple ANNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHPnGTMf95oI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## User Settings ########\n",
        "#### 1. Select parameters to be used ####\n",
        "input_var = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
        "#### 2. Select stations to be predicted ####\n",
        "# choose one of 'Emmaton', 'Jersey Point', 'Collinsville', 'Rock Slough', 'Antioch', 'Mallard',\n",
        "# 'LosVaqueros', 'Martinez', 'MiddleRiver', 'Vict Intake', 'CVP Intake', 'CCFB_OldR'\n",
        "output_stations=['CCFB_OldR']\n",
        "#### 3. Specify directory to excel dataset and the helper script (folder name only) ####\n",
        "#### if data is uploaded instead of in your google drive, set data_in_google_drive=False #####\n",
        "data_in_google_drive = True\n",
        "google_drive_dir = 'python_ANN'\n",
        "\n",
        "output_shape = 1\n",
        "nn_shape = [None,8*output_shape,2*output_shape,1*output_shape] # number of neurons in each layer\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Wd7z3l996j",
        "colab_type": "text"
      },
      "source": [
        "##1. Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6GxpZSUj0dF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "762c341f-f342-46a5-94a6-757fe9aaff66"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "# Mount Google drive\n",
        "if data_in_google_drive:\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    xl_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"ANN_data2.xlsx\")\n",
        "    %tensorflow_version 1.x\n",
        "else:\n",
        "    xl_path = \"ANN_data2.xlsx\"\n",
        "\n",
        "# import helper functions\n",
        "import sys\n",
        "!export PYTHONPATH=\"\"\n",
        "sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir))\n",
        "from ann_helper import read_data,normalize_in,writeF90,initnw,show_eval"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6348s2lzcfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "50896b4a-cb85-4a2f-dd73-3ddb99297719"
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "# from sklearn.utils import shuffle\n",
        "from scipy import stats\n",
        "\n",
        "test_mode = False\n",
        "\n",
        "locs = {'Emmaton':0,'Jersey Point':1,'Collinsville':2,'Rock Slough':3,'Antioch':4,\n",
        "        'Mallard':5, 'LosVaqueros':6, 'Martinez':7, 'MiddleRiver':8, 'Vict Intake':9,\n",
        "        'CVP Intake':10, 'CCFB_OldR':11}\n",
        "abbrev_map = {'rock slough':'ORRSL','rockslough':'ORRSL',\n",
        "            'emmaton':'EMM','jersey point':'JP','jerseypoint':'JP',\n",
        "            'antioch':'antioch','collinsville':'CO',\n",
        "            'mallard':'Mallard','mallard island':'Mallard',\n",
        "            'los vaqueros':'LosVaqueros','losvaqueros':'LosVaqueros',\n",
        "            'martinez':'MTZ',\n",
        "            'middle river':'MidR_intake','MiddleRiver':'MidR_intake',\n",
        "            'victoria cannal':'Victoria_intake','Vict Intake':'Victoria_intake',\n",
        "            'cvp intake':'CVP_intake','clfct forebay':'CCFB',\n",
        "            'clfct forebay intake':'CCFB_intake','x2':'X2'};\n",
        "\n",
        "output_stations = sorted(output_stations,key=lambda x: locs[x])\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "on_server = True\n",
        "\n",
        "# LM optimizer settings\n",
        "max_fail = 3\n",
        "epochs = 10\n",
        "init_mu = .05\n",
        "mu_max = 1e10\n",
        "target_mse = 0.\n",
        "\n",
        "# adam optimizer settings\n",
        "adam_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "if test_mode or device_name != '/device:GPU:0':\n",
        "    epochs = 5\n",
        "else:\n",
        "    epochs = 100\n",
        "\n",
        "train_loc = locs[output_stations[0]]\n",
        "if output_stations[0].lower() in abbrev_map.keys():\n",
        "    ann_name = abbrev_map[output_stations[0].lower()] \n",
        "else:\n",
        "    ann_name = output_stations[0].lower()\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# read data from excel\n",
        "x_data,y_data = read_data(xl_path,input_var,output_stations)\n",
        "end = time.time()\n",
        "print(\"loading data in %.2f seconds\" % (end-start) )\n",
        "    \n",
        "# normalize data to 0.1 ~ 0.9\n",
        "[x_norm,x_slope,x_bias] = normalize_in(x_data)\n",
        "[y_norm,y_slope,y_bias] = normalize_in(y_data)\n",
        "\n",
        "# split 80% for training, 20% for testing\n",
        "x_train_ori, x_test_ori, y_train0, y_test0 = train_test_split(x_norm,\n",
        "                                                              y_norm,\n",
        "                                                              test_size=0.2,\n",
        "                                                              random_state = 0)\n",
        "\n",
        "if test_mode:\n",
        "    x_train_ori = x_train_ori[:100]\n",
        "    x_test_ori = x_test_ori[:100]\n",
        "    y_train0 = y_train0[:100]\n",
        "    y_test0 = y_test0[:100]\n",
        "train_err = []\n",
        "test_err = []\n",
        "train_shape = len(x_train_ori)\n",
        "nn_shape[0] = x_train_ori.shape[1]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Disgarding last 1 row(s) of data in output set...\n",
            "loading data in 43.95 seconds\n",
            "loading data in 43.99 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZNeDaHG0fyp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1506ac75-7050-44e3-9d4d-3a0d92e9869d"
      },
      "source": [
        "# build artificial neural network\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, nn_shape[0]], name='InputData')\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, 1], name='LabelData')\n",
        "\n",
        "init_val = initnw(list(zip(*(nn_shape[i:] for i in range(2)))),x_train_ori)\n",
        "\n",
        "W1 = tf.Variable(initial_value=init_val[0][0], name='w1',dtype='float32')\n",
        "b1 = tf.Variable(initial_value=init_val[0][1], name='b1',dtype='float32')\n",
        "W2 = tf.Variable(initial_value=init_val[1][0], name='w2',dtype='float32')\n",
        "b2 = tf.Variable(initial_value=init_val[1][1], name='b2',dtype='float32')\n",
        "W3 = tf.Variable(initial_value=init_val[2][0], name='w3',dtype='float32')\n",
        "b3 = tf.Variable(initial_value=init_val[2][1], name='b3',dtype='float32')\n",
        "\n",
        "with tf.compat.v1.name_scope('layer1'):\n",
        "    first_out = tf.sigmoid(tf.add(tf.matmul(x,W1),b1))\n",
        "with tf.compat.v1.name_scope('layer2'):\n",
        "    second_out = tf.sigmoid(tf.add(tf.matmul(first_out,W2),b2))\n",
        "# with tf.name_scope('layer3'):\n",
        "#     third_out = tf.sigmoid(tf.add(tf.matmul(second_out,W3),b3))\n",
        "with tf.compat.v1.name_scope('layer3'):\n",
        "    pred = tf.matmul(second_out, W3) + b3\n",
        "\n",
        "# residuals\n",
        "r = tf.subtract(pred,y)\n",
        "# loss function must be mse or sse for LM\n",
        "cost = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "\n",
        "# For adam optimizer:\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "initial_lr = 1e-2\n",
        "learning_rate = tf.compat.v1.train.exponential_decay(initial_lr, global_step,\n",
        "                                       40, 0.999, staircase=True)\n",
        "# Adam optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    adam_opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step = global_step)\n",
        "\n",
        "# LM optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=1)\n",
        "\n",
        "with tf.compat.v1.name_scope('Accuracy'):\n",
        "    # Accuracy\n",
        "    acc = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "\n",
        "# LM algorithm\n",
        "def jacobian(y, x):\n",
        "    loop_vars = [\n",
        "        tf.constant(0, tf.int32),\n",
        "        tf.TensorArray(tf.float32, size=train_shape),\n",
        "    ]\n",
        "\n",
        "    _, jacobian = tf.while_loop(\n",
        "        cond=lambda i, _: i < train_shape,\n",
        "        body=lambda i, res: (i+1, res.write(i, tf.reshape(tf.gradients(ys=y[i], xs=x), (-1,)))),\n",
        "        loop_vars=loop_vars)\n",
        "    return jacobian.stack()\n",
        "\n",
        "\n",
        "r_flat = tf.expand_dims(tf.reshape(r,[-1]),1)\n",
        "parms = [W1, b1, W2, b2, W3, b3]\n",
        "parms_sizes = [tf.size(input=p) for p in parms]\n",
        "j = tf.concat([jacobian(r_flat, p) for p in parms], 1)\n",
        "jT = tf.transpose(a=j)\n",
        "hess_approx = tf.matmul(jT, j)\n",
        "grad_approx = tf.matmul(jT, r_flat)\n",
        "\n",
        "mu = tf.compat.v1.placeholder(tf.float32, shape=[])\n",
        "\n",
        "store = [tf.Variable(tf.zeros(p.shape, dtype=tf.float32)) for p in parms]\n",
        "save_parms = [tf.compat.v1.assign(s, p) for s, p in zip(store, parms)]\n",
        "restore_parms = [tf.compat.v1.assign(p, s) for s, p in zip(store, parms)]\n",
        "\n",
        "wb_flat = tf.concat([tf.reshape(p,[-1,1]) for p in parms],axis=0)\n",
        "n = tf.add_n(parms_sizes)\n",
        "I = tf.eye(n, dtype=tf.float32)\n",
        "w_2 = tf.reduce_sum(input_tensor=tf.square(wb_flat))\n",
        "\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(init)\n",
        "print(\"Number of trainable parameters: %d\" % sess.run(n))\n",
        "\n",
        "\n",
        "#  lm algorithm\n",
        "dp_flat = tf.matmul(tf.linalg.inv(hess_approx + tf.multiply(mu, I)), grad_approx)\n",
        "dps = tf.split(dp_flat, parms_sizes, 0)\n",
        "\n",
        "for i in range(len(dps)):\n",
        "    dps[i] = tf.reshape(dps[i], parms[i].shape)\n",
        "lm = opt.apply_gradients(zip(dps, parms))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 1037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgyRTmYsLR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.compat.v1.train.Saver({'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3})"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy1XKPUeKd10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adam optimizer training\n",
        "\n",
        "\n",
        "# Feed batch data\n",
        "def get_batch(inputX, inputY, batch_size):\n",
        "    duration = len(inputX)\n",
        "    for i in range(0,duration//batch_size):\n",
        "        idx = i*batch_size\n",
        "        yield inputX[idx:idx+batch_size], inputY[idx:idx+batch_size]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Start training\n",
        "\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        cost_list = []\n",
        "        lr_list = []\n",
        "        # Run the initializer\n",
        "        sess.run(init)\n",
        "\n",
        "        # Training cycle\n",
        "        total_batch = train_shape//batch_size\n",
        "        for epoch in range(adam_epochs):\n",
        "            avg_cost = 0.\n",
        "            ii = 0\n",
        "            # Loop over all batches\n",
        "            for batch_xs, batch_ys in get_batch(x_train_ori, y_train0,batch_size):\n",
        "                ii = ii +1\n",
        "                # Run optimization op (backprop), cost op (to get loss value)\n",
        "                # and summary nodes\n",
        "                _, c = sess.run([adam_opt, cost],\n",
        "                                 feed_dict={x: batch_xs, y: batch_ys})\n",
        "                # Write logs at every iteration\n",
        "                # Compute average loss\n",
        "                avg_cost += c / total_batch\n",
        "            lr = learning_rate.eval()\n",
        "            cost_list.append(avg_cost)\n",
        "            lr_list.append(lr)\n",
        "            # Display logs per epoch step\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost),\n",
        "                  \"lr = \",\"{:.10f}\".format(lr))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    # Calculate accuracy\n",
        "    print(\"Train Error:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "    print(\"Test Error:\", acc.eval({x: x_test_ori, y: y_test0}))\n",
        "    y_test_ = sess.run(pred,feed_dict={x:x_test_ori})\n",
        "    print(\"Test MSE:\", np.mean(np.square((y_test_-y_test0)/y_slope)))\n",
        "    print(\"Test MAPE:\", np.mean(np.abs(y_test_-y_test0)/y_test0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw49GOpORTqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "654e7b20-c8eb-4567-94b0-a75a558be0a3"
      },
      "source": [
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        print(\"Train Error before LM:\", acc.eval({x: x_train_ori, y: y_train0}))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Error before LM: 0.0048930175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FrIJ-IAfF9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c3a95e8-531d-486f-d4cc-91ebfe9dd4c4"
      },
      "source": [
        "\n",
        "feed_dict = {x: x_train_ori,\n",
        "             y: y_train0}\n",
        "feed_dict[mu] = init_mu\n",
        "\n",
        "# construct so-called feed dictionary to map placeholders to actual values\n",
        "\n",
        "validation_feed_dict = {x: x_test_ori,\n",
        "                        y: y_test0}\n",
        "\n",
        "train_break = False\n",
        "\n",
        "current_loss = sess.run(cost,{x: x_train_ori,y: y_train0})\n",
        "\n",
        "\n",
        "# Start training\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        epoch = 1\n",
        "        fail_step = 0\n",
        "        # Training cycle\n",
        "        while epoch < epochs and current_loss > target_mse:\n",
        "            if not(epoch%2) or epoch == 1:\n",
        "                val_loss = sess.run(cost, validation_feed_dict)\n",
        "                print('epoch: %3d , mu: %.5e, train loss: %.10f, val loss: %.10f'%(epoch,feed_dict[mu],current_loss,val_loss))\n",
        "            sess.run(save_parms)\n",
        "            while True:\n",
        "                start_step = time.time()\n",
        "                sess.run(lm, feed_dict)\n",
        "                new_loss = sess.run(cost,feed_dict)\n",
        "                # print('One update ended in %d seconds' % (time.time()-start_step))\n",
        "                if new_loss > current_loss:\n",
        "                    fail_step += 1\n",
        "                    feed_dict[mu] *= 10\n",
        "                    if feed_dict[mu] > mu_max or fail_step > max_fail:\n",
        "                        train_break = True\n",
        "                        break\n",
        "                    sess.run(restore_parms)\n",
        "                else:\n",
        "                    print(\"mu: %.5e, new loss: %.5f, previous loss: %.5f\"%(feed_dict[mu],new_loss,current_loss))\n",
        "                    fail_step = 0\n",
        "                    feed_dict[mu] /= 10\n",
        "                    feed_dict[mu] = max(1e-20,feed_dict[mu])\n",
        "                    current_loss = new_loss\n",
        "                    break\n",
        "            if train_break:\n",
        "                print('Failed for %d step(s), current mu = %.5e, stop training' % (fail_step,feed_dict[mu]))\n",
        "                break\n",
        "            epoch += 1\n",
        "    print(\"Location #%d Optimization Finished in %d seconds!\" %(train_loc, time.time() - start))\n",
        "\n",
        "    # Test model\n",
        "\n",
        "    y_train_predicted = sess.run(pred,feed_dict)\n",
        "    y_test_predicted = sess.run(pred,validation_feed_dict)\n",
        "\n",
        "    show_eval(y_train_predicted[:,0],\n",
        "              y_train0[:,0],\n",
        "              y_test_predicted[:,0],\n",
        "              y_test0[:,0],\n",
        "              y_slope,y_bias,ann_name)\n",
        "    writeF90(ann_name,\n",
        "             y_slope[0],y_bias[0],\n",
        "             sess.run(W1).transpose(),sess.run(b1),\n",
        "             sess.run(W2).transpose(),sess.run(b2),\n",
        "             sess.run(W3).transpose(),sess.run(b3))\n",
        "\n",
        "model_path = \"./%s/model.ckpt\" % (ann_name)\n",
        "save_path = saver.save(sess, model_path)\n",
        "print(\"Model saved in path: %s\" % save_path)\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        saved_test_loss = acc.eval(validation_feed_dict)\n",
        "        print(\"Train Error after LM:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "        print(\"Test Error after LM:\", saved_test_loss)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:   1 , mu: 5.00000e-02, train loss: 0.0048930175, val loss: 0.0050581386\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}