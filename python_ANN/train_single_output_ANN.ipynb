{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_single_output_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbLsjPUk3oIb"
      },
      "source": [
        "## Training a Single-Output ANN from scratch\n",
        "\n",
        "Note: this script trains only one ANN for the station selected by user. Please set 'output_stations' to different stations for training multiple ANNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHPnGTMf95oI"
      },
      "source": [
        "######## User Settings ########\n",
        "\n",
        "#### 1. Select parameters to be used ####\n",
        "input_var = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
        "#### 2. Select stations to be predicted ####\n",
        "# choose ONE of 'Emmaton','Jersey Point','Collinsville','Rock Slough'\n",
        "output_stations=['Rock Slough']\n",
        "#### 3. Specify directory to excel dataset and the helper script (folder name only) ####\n",
        "google_drive_dir = 'python_ANN'\n",
        "\n",
        "###### User Settings Finished ######\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Wd7z3l996j"
      },
      "source": [
        "##1. Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6GxpZSUj0dF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad322688-dcef-4e43-b1c3-8ff361093ea0"
      },
      "source": [
        "##  import helper functions\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Mount Google drive\n",
        "data_in_google_drive = True\n",
        "\n",
        "if data_in_google_drive:\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    xl_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"ANN_data.xlsx\")\n",
        "    %tensorflow_version 1.x\n",
        "else:\n",
        "    xl_path = \"ANN_data.xlsx\"\n",
        "\n",
        "sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir))\n",
        "from ann_helper import read_data,normalize_in,writeF90,initnw,show_eval"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6348s2lzcfC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c3aa18-a187-475c-86cb-6808cfe6e4a0"
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "# from sklearn.utils import shuffle\n",
        "from scipy import stats\n",
        "\n",
        "test_mode = False\n",
        "\n",
        "locs = {'Emmaton':0,'Jersey Point':1,'Collinsville':2,'Rock Slough':3}\n",
        "abbrev_map = {'rock slough':'ORRSL','rockslough':'ORRSL',\n",
        "            'emmaton':'EMM','jersey point':'JP','jerseypoint':'JP',\n",
        "            'antioch':'antioch','collinsville':'CO',\n",
        "            'mallard':'Mallard','mallard island':'Mallard',\n",
        "            'los vaqueros':'LosVaqueros','losvaqueros':'LosVaqueros',\n",
        "            'martinez':'MTZ',\n",
        "            'middle river':'MidR_intake','MiddleRiver':'MidR_intake',\n",
        "            'victoria cannal':'Victoria_intake','Vict Intake':'Victoria_intake',\n",
        "            'cvp intake':'CVP_intake','clfct forebay':'CCFB',\n",
        "            'clfct forebay intake':'CCFB_intake','x2':'X2'};\n",
        "\n",
        "output_stations = sorted(output_stations,key=lambda x: locs[x])\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "#   print('xxxxxxxxxxxxxx Using CPU xxxxxxxxxxxxxx')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "input_shape = (1,17*len(input_var))\n",
        "output_shape = 1\n",
        "nn_shape = [17*len(input_var),8,2,1] # number of neurons in each layer\n",
        "on_server = True\n",
        "\n",
        "# LM optimizer settings\n",
        "max_fail = 3\n",
        "epochs = 10\n",
        "init_mu = .05\n",
        "mu_max = 1e10\n",
        "target_mse = 0.\n",
        "\n",
        "# adam optimizer settings\n",
        "adam_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "if test_mode or device_name != '/device:GPU:0':\n",
        "    epochs = 5\n",
        "else:\n",
        "    epochs = 100\n",
        "\n",
        "train_loc = locs[output_stations[0]]\n",
        "ann_name = abbrev_map[output_stations[0].lower()]\n",
        "start = time.time()\n",
        "\n",
        "# read data from excel\n",
        "x_data,y_data = read_data(xl_path,input_var,output_stations)\n",
        "end = time.time()\n",
        "print(\"loading data in %.2f seconds\" % (end-start) )\n",
        "    \n",
        "# normalize data to 0.1 ~ 0.9\n",
        "[x_norm,x_slope,x_bias] = normalize_in(x_data)\n",
        "[y_norm,y_slope,y_bias] = normalize_in(y_data)\n",
        "\n",
        "# split 80% for training, 20% for testing\n",
        "x_train_ori, x_test_ori, y_train0, y_test0 = train_test_split(x_norm,\n",
        "                                                              y_norm,\n",
        "                                                              test_size=0.2,\n",
        "                                                              random_state = 0)\n",
        "\n",
        "if test_mode:\n",
        "    x_train_ori = x_train_ori[:100]\n",
        "    x_test_ori = x_test_ori[:100]\n",
        "    y_train0 = y_train0[:100]\n",
        "    y_test0 = y_test0[:100]\n",
        "train_err = []\n",
        "test_err = []\n",
        "train_shape = len(x_train_ori)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Disgarding last 1 row(s) of data in output set...\n",
            "loading data in 28.70 seconds\n",
            "loading data in 28.73 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZNeDaHG0fyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e245a77-f5e0-481b-e3c4-11b5751a2619"
      },
      "source": [
        "# build artificial neural network\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, 17*len(input_var)], name='InputData')\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, 1], name='LabelData')\n",
        "\n",
        "init_val = initnw(list(zip(*(nn_shape[i:] for i in range(2)))),x_train_ori)\n",
        "\n",
        "W1 = tf.Variable(initial_value=init_val[0][0], name='w1',dtype='float32')\n",
        "b1 = tf.Variable(initial_value=init_val[0][1], name='b1',dtype='float32')\n",
        "W2 = tf.Variable(initial_value=init_val[1][0], name='w2',dtype='float32')\n",
        "b2 = tf.Variable(initial_value=init_val[1][1], name='b2',dtype='float32')\n",
        "W3 = tf.Variable(initial_value=init_val[2][0], name='w3',dtype='float32')\n",
        "b3 = tf.Variable(initial_value=init_val[2][1], name='b3',dtype='float32')\n",
        "\n",
        "with tf.compat.v1.name_scope('layer1'):\n",
        "    first_out = tf.sigmoid(tf.add(tf.matmul(x,W1),b1))\n",
        "with tf.compat.v1.name_scope('layer2'):\n",
        "    second_out = tf.sigmoid(tf.add(tf.matmul(first_out,W2),b2))\n",
        "# with tf.name_scope('layer3'):\n",
        "#     third_out = tf.sigmoid(tf.add(tf.matmul(second_out,W3),b3))\n",
        "with tf.compat.v1.name_scope('layer3'):\n",
        "    pred = tf.matmul(second_out, W3) + b3\n",
        "\n",
        "# residuals\n",
        "r = tf.subtract(pred,y)\n",
        "# loss function must be mse or sse for LM\n",
        "cost = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "\n",
        "# For adam optimizer:\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "initial_lr = 1e-2\n",
        "learning_rate = tf.compat.v1.train.exponential_decay(initial_lr, global_step,\n",
        "                                       40, 0.999, staircase=True)\n",
        "# Adam optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    adam_opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step = global_step)\n",
        "\n",
        "# LM optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=1)\n",
        "\n",
        "with tf.compat.v1.name_scope('Accuracy'):\n",
        "    # Accuracy\n",
        "    acc = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "\n",
        "# LM algorithm\n",
        "def jacobian(y, x):\n",
        "    loop_vars = [\n",
        "        tf.constant(0, tf.int32),\n",
        "        tf.TensorArray(tf.float32, size=train_shape),\n",
        "    ]\n",
        "\n",
        "    _, jacobian = tf.while_loop(\n",
        "        cond=lambda i, _: i < train_shape,\n",
        "        body=lambda i, res: (i+1, res.write(i, tf.reshape(tf.gradients(ys=y[i], xs=x), (-1,)))),\n",
        "        loop_vars=loop_vars)\n",
        "    return jacobian.stack()\n",
        "\n",
        "\n",
        "r_flat = tf.expand_dims(tf.reshape(r,[-1]),1)\n",
        "parms = [W1, b1, W2, b2, W3, b3]\n",
        "parms_sizes = [tf.size(input=p) for p in parms]\n",
        "j = tf.concat([jacobian(r_flat, p) for p in parms], 1)\n",
        "jT = tf.transpose(a=j)\n",
        "hess_approx = tf.matmul(jT, j)\n",
        "grad_approx = tf.matmul(jT, r_flat)\n",
        "\n",
        "mu = tf.compat.v1.placeholder(tf.float32, shape=[])\n",
        "\n",
        "store = [tf.Variable(tf.zeros(p.shape, dtype=tf.float32)) for p in parms]\n",
        "save_parms = [tf.compat.v1.assign(s, p) for s, p in zip(store, parms)]\n",
        "restore_parms = [tf.compat.v1.assign(p, s) for s, p in zip(store, parms)]\n",
        "\n",
        "wb_flat = tf.concat([tf.reshape(p,[-1,1]) for p in parms],axis=0)\n",
        "n = tf.add_n(parms_sizes)\n",
        "I = tf.eye(n, dtype=tf.float32)\n",
        "w_2 = tf.reduce_sum(input_tensor=tf.square(wb_flat))\n",
        "\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(init)\n",
        "print(\"Number of trainable parameters: %d\" % sess.run(n))\n",
        "\n",
        "\n",
        "#  lm algorithm\n",
        "dp_flat = tf.matmul(tf.linalg.inv(hess_approx + tf.multiply(mu, I)), grad_approx)\n",
        "dps = tf.split(dp_flat, parms_sizes, 0)\n",
        "\n",
        "for i in range(len(dps)):\n",
        "    dps[i] = tf.reshape(dps[i], parms[i].shape)\n",
        "lm = opt.apply_gradients(zip(dps, parms))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgyRTmYsLR4"
      },
      "source": [
        "saver = tf.compat.v1.train.Saver({'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3})"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy1XKPUeKd10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "461fed9a-860c-4571-b001-d0f6b812f776"
      },
      "source": [
        "# Adam optimizer training\n",
        "\n",
        "\n",
        "# Feed batch data\n",
        "def get_batch(inputX, inputY, batch_size):\n",
        "    duration = len(inputX)\n",
        "    for i in range(0,duration//batch_size):\n",
        "        idx = i*batch_size\n",
        "        yield inputX[idx:idx+batch_size], inputY[idx:idx+batch_size]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Start training\n",
        "\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        cost_list = []\n",
        "        lr_list = []\n",
        "        # Run the initializer\n",
        "        sess.run(init)\n",
        "\n",
        "        # Training cycle\n",
        "        total_batch = train_shape//batch_size\n",
        "        for epoch in range(adam_epochs):\n",
        "            avg_cost = 0.\n",
        "            ii = 0\n",
        "            # Loop over all batches\n",
        "            for batch_xs, batch_ys in get_batch(x_train_ori, y_train0,batch_size):\n",
        "                ii = ii +1\n",
        "                # Run optimization op (backprop), cost op (to get loss value)\n",
        "                # and summary nodes\n",
        "                _, c = sess.run([adam_opt, cost],\n",
        "                                 feed_dict={x: batch_xs, y: batch_ys})\n",
        "                # Write logs at every iteration\n",
        "                # Compute average loss\n",
        "                avg_cost += c / total_batch\n",
        "            lr = learning_rate.eval()\n",
        "            cost_list.append(avg_cost)\n",
        "            lr_list.append(lr)\n",
        "            # Display logs per epoch step\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost),\n",
        "                  \"lr = \",\"{:.10f}\".format(lr))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    # Calculate accuracy\n",
        "    print(\"Train Error:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "    print(\"Test Error:\", acc.eval({x: x_test_ori, y: y_test0}))\n",
        "    y_test_ = sess.run(pred,feed_dict={x:x_test_ori})\n",
        "    print(\"Test MSE:\", np.mean(np.square((y_test_-y_test0)/y_slope)))\n",
        "    print(\"Test MAPE:\", np.mean(np.abs(y_test_-y_test0)/y_test0))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 0.005301149 lr =  0.0098313550\n",
            "Epoch: 0002 cost= 0.003091346 lr =  0.0096655544\n",
            "Epoch: 0003 cost= 0.002619291 lr =  0.0095025497\n",
            "Epoch: 0004 cost= 0.002350847 lr =  0.0093422951\n",
            "Epoch: 0005 cost= 0.002165069 lr =  0.0091847414\n",
            "Epoch: 0006 cost= 0.002027307 lr =  0.0090298457\n",
            "Epoch: 0007 cost= 0.001917612 lr =  0.0088775624\n",
            "Epoch: 0008 cost= 0.001830566 lr =  0.0087278476\n",
            "Epoch: 0009 cost= 0.001751097 lr =  0.0085806567\n",
            "Epoch: 0010 cost= 0.001678437 lr =  0.0084275128\n",
            "Epoch: 0011 cost= 0.001616798 lr =  0.0082853874\n",
            "Epoch: 0012 cost= 0.001562008 lr =  0.0081456583\n",
            "Epoch: 0013 cost= 0.001512820 lr =  0.0080082864\n",
            "Epoch: 0014 cost= 0.001469247 lr =  0.0078732315\n",
            "Epoch: 0015 cost= 0.001430555 lr =  0.0077404534\n",
            "Epoch: 0016 cost= 0.001396032 lr =  0.0076099150\n",
            "Epoch: 0017 cost= 0.001364793 lr =  0.0074815778\n",
            "Epoch: 0018 cost= 0.001335890 lr =  0.0073554050\n",
            "Epoch: 0019 cost= 0.001308900 lr =  0.0072313598\n",
            "Epoch: 0020 cost= 0.001283832 lr =  0.0071022981\n",
            "Epoch: 0021 cost= 0.001260643 lr =  0.0069825212\n",
            "Epoch: 0022 cost= 0.001239110 lr =  0.0068647647\n",
            "Epoch: 0023 cost= 0.001218987 lr =  0.0067489943\n",
            "Epoch: 0024 cost= 0.001200056 lr =  0.0066351760\n",
            "Epoch: 0025 cost= 0.001182136 lr =  0.0065232776\n",
            "Epoch: 0026 cost= 0.001165106 lr =  0.0064132661\n",
            "Epoch: 0027 cost= 0.001148870 lr =  0.0063051092\n",
            "Epoch: 0028 cost= 0.001133309 lr =  0.0061987774\n",
            "Epoch: 0029 cost= 0.001118346 lr =  0.0060942383\n",
            "Epoch: 0030 cost= 0.001103966 lr =  0.0059854710\n",
            "Epoch: 0031 cost= 0.001090098 lr =  0.0058845291\n",
            "Epoch: 0032 cost= 0.001076706 lr =  0.0057852892\n",
            "Epoch: 0033 cost= 0.001063747 lr =  0.0056877243\n",
            "Epoch: 0034 cost= 0.001051201 lr =  0.0055918032\n",
            "Epoch: 0035 cost= 0.001039046 lr =  0.0054975008\n",
            "Epoch: 0036 cost= 0.001027270 lr =  0.0054047885\n",
            "Epoch: 0037 cost= 0.001015885 lr =  0.0053136395\n",
            "Epoch: 0038 cost= 0.001004848 lr =  0.0052240281\n",
            "Epoch: 0039 cost= 0.000994137 lr =  0.0051359269\n",
            "Epoch: 0040 cost= 0.000983377 lr =  0.0050442643\n",
            "Epoch: 0041 cost= 0.000970656 lr =  0.0049591945\n",
            "Epoch: 0042 cost= 0.000955867 lr =  0.0048755603\n",
            "Epoch: 0043 cost= 0.000943094 lr =  0.0047933366\n",
            "Epoch: 0044 cost= 0.000931378 lr =  0.0047124997\n",
            "Epoch: 0045 cost= 0.000919996 lr =  0.0046330262\n",
            "Epoch: 0046 cost= 0.000909034 lr =  0.0045548924\n",
            "Epoch: 0047 cost= 0.000898573 lr =  0.0044780769\n",
            "Epoch: 0048 cost= 0.000889024 lr =  0.0044025565\n",
            "Epoch: 0049 cost= 0.000879936 lr =  0.0043283100\n",
            "Epoch: 0050 cost= 0.000871319 lr =  0.0042510596\n",
            "Epoch: 0051 cost= 0.000863111 lr =  0.0041793678\n",
            "Epoch: 0052 cost= 0.000854383 lr =  0.0041088853\n",
            "Epoch: 0053 cost= 0.000848675 lr =  0.0040395907\n",
            "Epoch: 0054 cost= 0.000842341 lr =  0.0039714654\n",
            "Epoch: 0055 cost= 0.000836282 lr =  0.0039044886\n",
            "Epoch: 0056 cost= 0.000829465 lr =  0.0038386418\n",
            "Epoch: 0057 cost= 0.000822334 lr =  0.0037739053\n",
            "Epoch: 0058 cost= 0.000814720 lr =  0.0037102599\n",
            "Epoch: 0059 cost= 0.000806998 lr =  0.0036476888\n",
            "Epoch: 0060 cost= 0.000799569 lr =  0.0035825865\n",
            "Epoch: 0061 cost= 0.000792511 lr =  0.0035221679\n",
            "Epoch: 0062 cost= 0.000785814 lr =  0.0034627684\n",
            "Epoch: 0063 cost= 0.000779398 lr =  0.0034043705\n",
            "Epoch: 0064 cost= 0.000773221 lr =  0.0033469577\n",
            "Epoch: 0065 cost= 0.000767230 lr =  0.0032905131\n",
            "Epoch: 0066 cost= 0.000761372 lr =  0.0032350207\n",
            "Epoch: 0067 cost= 0.000755676 lr =  0.0031804633\n",
            "Epoch: 0068 cost= 0.000750102 lr =  0.0031268266\n",
            "Epoch: 0069 cost= 0.000744619 lr =  0.0030740942\n",
            "Epoch: 0070 cost= 0.000739245 lr =  0.0030192290\n",
            "Epoch: 0071 cost= 0.000733959 lr =  0.0029683115\n",
            "Epoch: 0072 cost= 0.000728771 lr =  0.0029182525\n",
            "Epoch: 0073 cost= 0.000723657 lr =  0.0028690377\n",
            "Epoch: 0074 cost= 0.000718628 lr =  0.0028206529\n",
            "Epoch: 0075 cost= 0.000713676 lr =  0.0027730847\n",
            "Epoch: 0076 cost= 0.000708777 lr =  0.0027263178\n",
            "Epoch: 0077 cost= 0.000703971 lr =  0.0026803399\n",
            "Epoch: 0078 cost= 0.000699250 lr =  0.0026351374\n",
            "Epoch: 0079 cost= 0.000694587 lr =  0.0025906970\n",
            "Epoch: 0080 cost= 0.000690015 lr =  0.0025444597\n",
            "Epoch: 0081 cost= 0.000685515 lr =  0.0025015485\n",
            "Epoch: 0082 cost= 0.000681106 lr =  0.0024593612\n",
            "Epoch: 0083 cost= 0.000676769 lr =  0.0024178852\n",
            "Epoch: 0084 cost= 0.000672511 lr =  0.0023771089\n",
            "Epoch: 0085 cost= 0.000668335 lr =  0.0023370204\n",
            "Epoch: 0086 cost= 0.000664222 lr =  0.0022976077\n",
            "Epoch: 0087 cost= 0.000660193 lr =  0.0022588600\n",
            "Epoch: 0088 cost= 0.000656254 lr =  0.0022207657\n",
            "Epoch: 0089 cost= 0.000652373 lr =  0.0021833135\n",
            "Epoch: 0090 cost= 0.000648582 lr =  0.0021443467\n",
            "Epoch: 0091 cost= 0.000644864 lr =  0.0021081835\n",
            "Epoch: 0092 cost= 0.000641231 lr =  0.0020726300\n",
            "Epoch: 0093 cost= 0.000637670 lr =  0.0020376761\n",
            "Epoch: 0094 cost= 0.000634185 lr =  0.0020033119\n",
            "Epoch: 0095 cost= 0.000630776 lr =  0.0019695272\n",
            "Epoch: 0096 cost= 0.000627435 lr =  0.0019363119\n",
            "Epoch: 0097 cost= 0.000624165 lr =  0.0019036572\n",
            "Epoch: 0098 cost= 0.000620979 lr =  0.0018715530\n",
            "Epoch: 0099 cost= 0.000617849 lr =  0.0018399904\n",
            "Epoch: 0100 cost= 0.000614801 lr =  0.0018071508\n",
            "Optimization Finished!\n",
            "Train Error: 0.0007146377\n",
            "Test Error: 0.00079483265\n",
            "Test MSE: 4320.33\n",
            "Test MAPE: 0.09326973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw49GOpORTqm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "857fc348-38ce-417c-9a1b-3b3a45e25a6e"
      },
      "source": [
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        print(\"Train Error before LM:\", acc.eval({x: x_train_ori, y: y_train0}))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Error before LM: 0.0007146377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FrIJ-IAfF9A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "513bb113-bbab-4831-e4d6-94529b2b6b65"
      },
      "source": [
        "\n",
        "feed_dict = {x: x_train_ori,\n",
        "             y: y_train0}\n",
        "feed_dict[mu] = init_mu\n",
        "\n",
        "# construct so-called feed dictionary to map placeholders to actual values\n",
        "\n",
        "validation_feed_dict = {x: x_test_ori,\n",
        "                        y: y_test0}\n",
        "\n",
        "train_break = False\n",
        "\n",
        "current_loss = sess.run(cost,{x: x_train_ori,y: y_train0})\n",
        "\n",
        "\n",
        "# Start training\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        epoch = 1\n",
        "        fail_step = 0\n",
        "        # Training cycle\n",
        "        while epoch < epochs and current_loss > target_mse:\n",
        "            if not(epoch%2) or epoch == 1:\n",
        "                val_loss = sess.run(cost, validation_feed_dict)\n",
        "                print('epoch: %3d , mu: %.5e, train loss: %.10f, val loss: %.10f'%(epoch,feed_dict[mu],current_loss,val_loss))\n",
        "            sess.run(save_parms)\n",
        "            while True:\n",
        "                start_step = time.time()\n",
        "                sess.run(lm, feed_dict)\n",
        "                new_loss = sess.run(cost,feed_dict)\n",
        "                # print('One update ended in %d seconds' % (time.time()-start_step))\n",
        "                if new_loss > current_loss:\n",
        "                    fail_step += 1\n",
        "                    feed_dict[mu] *= 10\n",
        "                    if feed_dict[mu] > mu_max or fail_step > max_fail:\n",
        "                        train_break = True\n",
        "                        break\n",
        "                    sess.run(restore_parms)\n",
        "                else:\n",
        "                    print(\"mu: %.5e, new loss: %.5f, previous loss: %.5f\"%(feed_dict[mu],new_loss,current_loss))\n",
        "                    fail_step = 0\n",
        "                    feed_dict[mu] /= 10\n",
        "                    feed_dict[mu] = max(1e-20,feed_dict[mu])\n",
        "                    current_loss = new_loss\n",
        "                    break\n",
        "            if train_break:\n",
        "                print('Failed for %d step(s), current mu = %.5e, stop training' % (fail_step,feed_dict[mu]))\n",
        "                break\n",
        "            epoch += 1\n",
        "    print(\"Location #%d Optimization Finished in %d seconds!\" %(train_loc, time.time() - start))\n",
        "\n",
        "    # Test model\n",
        "\n",
        "    y_train_predicted = sess.run(pred,feed_dict)\n",
        "    y_test_predicted = sess.run(pred,validation_feed_dict)\n",
        "\n",
        "    show_eval(y_train_predicted[:,0],\n",
        "              y_train0[:,0],\n",
        "              y_test_predicted[:,0],\n",
        "              y_test0[:,0],\n",
        "              y_slope,y_bias,ann_name)\n",
        "    writeF90('.',abbrev_map[output_stations[0].lower()],\n",
        "             y_slope[0],y_bias[0],\n",
        "             sess.run(W1).transpose(),sess.run(b1),\n",
        "             sess.run(W2).transpose(),sess.run(b2),\n",
        "             sess.run(W3).transpose(),sess.run(b3))\n",
        "\n",
        "model_path = \"./%s/model.ckpt\" % (abbrev_map[output_stations[0].lower()])\n",
        "save_path = saver.save(sess, model_path)\n",
        "print(\"Model saved in path: %s\" % save_path)\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        saved_test_loss = acc.eval(validation_feed_dict)\n",
        "        print(\"Train Error after LM:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "        print(\"Test Error after LM:\", saved_test_loss)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:   1 , mu: 5.00000e-02, train loss: 0.0007146377, val loss: 0.0007948327\n",
            "mu: 5.00000e-02, new loss: 0.00056, previous loss: 0.00071\n",
            "epoch:   2 , mu: 5.00000e-03, train loss: 0.0005586084, val loss: 0.0006467368\n",
            "mu: 5.00000e-02, new loss: 0.00055, previous loss: 0.00056\n",
            "mu: 5.00000e-02, new loss: 0.00040, previous loss: 0.00055\n",
            "epoch:   4 , mu: 5.00000e-03, train loss: 0.0004036136, val loss: 0.0004750810\n",
            "mu: 5.00000e-02, new loss: 0.00038, previous loss: 0.00040\n",
            "mu: 5.00000e-02, new loss: 0.00036, previous loss: 0.00038\n",
            "epoch:   6 , mu: 5.00000e-03, train loss: 0.0003634893, val loss: 0.0004320159\n",
            "mu: 5.00000e-03, new loss: 0.00035, previous loss: 0.00036\n",
            "mu: 5.00000e-03, new loss: 0.00030, previous loss: 0.00035\n",
            "epoch:   8 , mu: 5.00000e-04, train loss: 0.0002994349, val loss: 0.0003692539\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00030\n",
            "mu: 5.00000e-03, new loss: 0.00027, previous loss: 0.00029\n",
            "epoch:  10 , mu: 5.00000e-04, train loss: 0.0002743753, val loss: 0.0003460014\n",
            "mu: 5.00000e-03, new loss: 0.00027, previous loss: 0.00027\n",
            "mu: 5.00000e-03, new loss: 0.00026, previous loss: 0.00027\n",
            "epoch:  12 , mu: 5.00000e-04, train loss: 0.0002586688, val loss: 0.0003300899\n",
            "mu: 5.00000e-03, new loss: 0.00025, previous loss: 0.00026\n",
            "mu: 5.00000e-03, new loss: 0.00025, previous loss: 0.00025\n",
            "epoch:  14 , mu: 5.00000e-04, train loss: 0.0002469275, val loss: 0.0003149962\n",
            "mu: 5.00000e-03, new loss: 0.00024, previous loss: 0.00025\n",
            "mu: 5.00000e-03, new loss: 0.00024, previous loss: 0.00024\n",
            "epoch:  16 , mu: 5.00000e-04, train loss: 0.0002367961, val loss: 0.0002995466\n",
            "mu: 5.00000e-03, new loss: 0.00023, previous loss: 0.00024\n",
            "mu: 5.00000e-04, new loss: 0.00023, previous loss: 0.00023\n",
            "epoch:  18 , mu: 5.00000e-05, train loss: 0.0002284619, val loss: 0.0002797096\n",
            "mu: 5.00000e-04, new loss: 0.00022, previous loss: 0.00023\n",
            "mu: 5.00000e-04, new loss: 0.00021, previous loss: 0.00022\n",
            "epoch:  20 , mu: 5.00000e-05, train loss: 0.0002094691, val loss: 0.0002542391\n",
            "mu: 5.00000e-04, new loss: 0.00020, previous loss: 0.00021\n",
            "mu: 5.00000e-04, new loss: 0.00020, previous loss: 0.00020\n",
            "epoch:  22 , mu: 5.00000e-05, train loss: 0.0001960561, val loss: 0.0002429142\n",
            "mu: 5.00000e-03, new loss: 0.00019, previous loss: 0.00020\n",
            "mu: 5.00000e-04, new loss: 0.00019, previous loss: 0.00019\n",
            "epoch:  24 , mu: 5.00000e-05, train loss: 0.0001925553, val loss: 0.0002394749\n",
            "mu: 5.00000e-04, new loss: 0.00019, previous loss: 0.00019\n",
            "mu: 5.00000e-04, new loss: 0.00019, previous loss: 0.00019\n",
            "epoch:  26 , mu: 5.00000e-05, train loss: 0.0001891523, val loss: 0.0002372556\n",
            "mu: 5.00000e-04, new loss: 0.00019, previous loss: 0.00019\n",
            "mu: 5.00000e-04, new loss: 0.00019, previous loss: 0.00019\n",
            "epoch:  28 , mu: 5.00000e-05, train loss: 0.0001856891, val loss: 0.0002357365\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00019\n",
            "mu: 5.00000e-04, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  30 , mu: 5.00000e-05, train loss: 0.0001834606, val loss: 0.0002349065\n",
            "mu: 5.00000e-04, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-04, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  32 , mu: 5.00000e-05, train loss: 0.0001808233, val loss: 0.0002329702\n",
            "mu: 5.00000e-04, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  34 , mu: 5.00000e-04, train loss: 0.0001791769, val loss: 0.0002305983\n",
            "mu: 5.00000e-04, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  36 , mu: 5.00000e-04, train loss: 0.0001777604, val loss: 0.0002275820\n",
            "mu: 5.00000e-04, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  38 , mu: 5.00000e-04, train loss: 0.0001764537, val loss: 0.0002248599\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  40 , mu: 5.00000e-04, train loss: 0.0001760316, val loss: 0.0002236806\n",
            "mu: 5.00000e-04, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00018\n",
            "epoch:  42 , mu: 5.00000e-05, train loss: 0.0001741033, val loss: 0.0002206744\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  44 , mu: 5.00000e-05, train loss: 0.0001712742, val loss: 0.0002179001\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  46 , mu: 5.00000e-05, train loss: 0.0001691356, val loss: 0.0002161194\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  48 , mu: 5.00000e-05, train loss: 0.0001675045, val loss: 0.0002152209\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  50 , mu: 5.00000e-05, train loss: 0.0001659151, val loss: 0.0002137750\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-04, new loss: 0.00016, previous loss: 0.00017\n",
            "epoch:  52 , mu: 5.00000e-05, train loss: 0.0001644734, val loss: 0.0002116002\n",
            "mu: 5.00000e-04, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  54 , mu: 5.00000e-04, train loss: 0.0001633509, val loss: 0.0002101023\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  56 , mu: 5.00000e-04, train loss: 0.0001629018, val loss: 0.0002089735\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  58 , mu: 5.00000e-04, train loss: 0.0001624202, val loss: 0.0002078771\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  60 , mu: 5.00000e-04, train loss: 0.0001618768, val loss: 0.0002067811\n",
            "mu: 5.00000e-04, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  62 , mu: 5.00000e-04, train loss: 0.0001600416, val loss: 0.0002042914\n",
            "mu: 5.00000e-04, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  64 , mu: 5.00000e-04, train loss: 0.0001580348, val loss: 0.0002011037\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-04, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  66 , mu: 5.00000e-05, train loss: 0.0001562840, val loss: 0.0001982759\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00016\n",
            "epoch:  68 , mu: 5.00000e-05, train loss: 0.0001549025, val loss: 0.0001955759\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-03, new loss: 0.00015, previous loss: 0.00015\n",
            "epoch:  70 , mu: 5.00000e-04, train loss: 0.0001525293, val loss: 0.0001929151\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "epoch:  72 , mu: 5.00000e-05, train loss: 0.0001504168, val loss: 0.0001887389\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-03, new loss: 0.00015, previous loss: 0.00015\n",
            "epoch:  74 , mu: 5.00000e-04, train loss: 0.0001494596, val loss: 0.0001872139\n",
            "mu: 5.00000e-03, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-03, new loss: 0.00015, previous loss: 0.00015\n",
            "epoch:  76 , mu: 5.00000e-04, train loss: 0.0001491319, val loss: 0.0001867424\n",
            "mu: 5.00000e-03, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "epoch:  78 , mu: 5.00000e-05, train loss: 0.0001488702, val loss: 0.0001848615\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-03, new loss: 0.00015, previous loss: 0.00015\n",
            "epoch:  80 , mu: 5.00000e-04, train loss: 0.0001474579, val loss: 0.0001837826\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-03, new loss: 0.00015, previous loss: 0.00015\n",
            "epoch:  82 , mu: 5.00000e-04, train loss: 0.0001466861, val loss: 0.0001824273\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "epoch:  84 , mu: 5.00000e-05, train loss: 0.0001459337, val loss: 0.0001807435\n",
            "mu: 5.00000e-03, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-03, new loss: 0.00015, previous loss: 0.00015\n",
            "epoch:  86 , mu: 5.00000e-04, train loss: 0.0001455140, val loss: 0.0001807208\n",
            "mu: 5.00000e-04, new loss: 0.00015, previous loss: 0.00015\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00015\n",
            "epoch:  88 , mu: 5.00000e-04, train loss: 0.0001449801, val loss: 0.0001798521\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "epoch:  90 , mu: 5.00000e-04, train loss: 0.0001448419, val loss: 0.0001796693\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "epoch:  92 , mu: 5.00000e-04, train loss: 0.0001447151, val loss: 0.0001794651\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "epoch:  94 , mu: 5.00000e-04, train loss: 0.0001445945, val loss: 0.0001792522\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "epoch:  96 , mu: 5.00000e-04, train loss: 0.0001444783, val loss: 0.0001790554\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "epoch:  98 , mu: 5.00000e-04, train loss: 0.0001443638, val loss: 0.0001788878\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "mu: 5.00000e-03, new loss: 0.00014, previous loss: 0.00014\n",
            "Location #3 Optimization Finished in 1476 seconds!\n",
            "train mape:  0.0495\n",
            "train mse:  784.10\n",
            "test mape:  0.0512\n",
            "test mse: 971.36\n",
            "<class 'numpy.float32'>\n",
            "Model saved in path: ./ORRSL/model.ckpt\n",
            "Train Error after LM: 0.00014425452\n",
            "Test Error after LM: 0.00017870577\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO3TvhKyz30q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59ff1215-208c-4435-b7a3-020d8c8a6ae8"
      },
      "source": [
        "## download fortran file and trained model\n",
        "\n",
        "model_dir = abbrev_map[output_stations[0].lower()]\n",
        "from google.colab import files\n",
        "ann_zip = ann_name +'.zip'\n",
        "f90name = 'fnet_'+ann_name+'.f90'\n",
        "!zip -r /content/$ann_zip $model_dir\n",
        "!zip -r /content/$ann_zip $f90name"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: ORRSL/ (stored 0%)\n",
            "updating: ORRSL/model.ckpt.index (deflated 32%)\n",
            "updating: ORRSL/model.ckpt.data-00000-of-00001 (deflated 5%)\n",
            "updating: ORRSL/model.ckpt.meta (deflated 86%)\n",
            "updating: ORRSL/checkpoint (deflated 42%)\n",
            "updating: fnet_ORRSL.f90 (deflated 55%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN1miFnUkHiE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2c84e07a-ef2e-402d-cd2f-83c97d035cd5"
      },
      "source": [
        "files.download('/content/%s' % ann_zip)\n",
        "# files.download('%s.jpg'%(output_stations[0]+str(fig_index)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_e010c102-133a-45d2-92d9-ddad87cf9a02\", \"ORRSL.zip\", 35221)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}