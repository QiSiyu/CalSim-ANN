{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTL_Conv_ANN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkXdH1XQM3j8"
      },
      "source": [
        "#------------------------------------------------------------------#\n",
        "#------------------------- User Settings --------------------------#\n",
        "#------------------------------------------------------------------#\n",
        "\n",
        "#### 1. Select one or more input variables from:\n",
        "# 'SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC'\n",
        "\n",
        "input_var = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
        "\n",
        "#### 2. Select one or more output stations from:\n",
        "# 'Emmaton', 'Jersey Point', 'Collinsville', 'Rock Slough', 'Antioch', 'Mallard',\n",
        "# 'LosVaqueros', 'Martinez', 'MiddleRiver', 'Vict Intake', 'CVP Intake', 'CCFB_OldR'\n",
        "\n",
        "output_stations=['Emmaton', 'Jersey Point', 'Collinsville', 'Rock Slough', 'Antioch', 'Mallard',\n",
        "                 'LosVaqueros', 'Martinez', 'MiddleRiver', 'Vict Intake', 'CVP Intake', 'CCFB_OldR']\n",
        "\n",
        "#### 3. Specify directory to excel dataset and the helper script (folder name only)\n",
        "google_drive_dir = 'python_ANN'\n",
        "\n",
        "#### 4. Specify number of nodes in hidden layers\n",
        "hidden_layer_nodes = [8*12, 2*12]\n",
        "\n",
        "#### 5. define lead days (days forecasting ahead) for each station\n",
        "lead_day = 0\n",
        "lead_day_for_station = {'Emmaton':lead_day,\n",
        "                        'Jersey Point':lead_day,\n",
        "                        'Collinsville':lead_day,\n",
        "                        'Rock Slough':lead_day,\n",
        "                        'Antioch':lead_day,\n",
        "                        'Mallard':lead_day,\n",
        "                        'LosVaqueros':lead_day,\n",
        "                        'Martinez':lead_day,\n",
        "                        'MiddleRiver':lead_day,\n",
        "                        'Vict Intake':lead_day,\n",
        "                        'CVP Intake':lead_day,\n",
        "                        'CCFB_OldR':lead_day}\n",
        "\n",
        "#------------------------------------------------------------------#\n",
        "#------------------- User Settings Finished -----------------------#\n",
        "#------------------------------------------------------------------#\n",
        "\n",
        "\n",
        "### set this to False if you want to train locally\n",
        "data_in_google_drive = True\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Mount Google drive\n",
        "if data_in_google_drive:\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    input_data_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"input_2.csv\")\n",
        "    output_data_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"output_2.csv\")\n",
        "else:\n",
        "    ### set to dataset path if want to train locally\n",
        "    input_data_path = '/Users/siyuqi/.spyder-py3/DSM2/input_2.csv'\n",
        "    output_data_path = '/Users/siyuqi/.spyder-py3/DSM2/output_2.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUaY9du2wmWw"
      },
      "source": [
        "Check user settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCzCj64kwmLl"
      },
      "source": [
        "full_input_variable_list = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
        "full_output_station_list = ['Emmaton', 'Jersey Point', 'Collinsville', 'Rock Slough', 'Antioch', 'Mallard',\n",
        "                            'LosVaqueros', 'Martinez', 'MiddleRiver', 'Vict Intake', 'CVP Intake', 'CCFB_OldR']\n",
        "\n",
        "# check if input variables are valid\n",
        "assert len(set(input_var))==len(input_var), ('Duplicate input variable(s): ' + \", \".join(str(e) for e in set([x for x in input_var if input_var.count(x) > 1])) )\n",
        "assert len(list(set(input_var).intersection(full_input_variable_list)))==len(input_var), ('Invalid input variable(s): ' + \", \".join(str(e) for e in set([x for x in input_var if full_input_variable_list.count(x) == 0])) )\n",
        "\n",
        "# check if output stations are valid\n",
        "assert len(set(output_stations))==len(output_stations), ('Duplicate output station(s): ' + \", \".join(str(e) for e in set([x for x in output_stations if output_stations.count(x) > 1])) )\n",
        "assert len(list(set(output_stations).intersection(full_output_station_list)))==len(output_stations), ('Invalid output station(s): ' + \", \".join(str(e) for e in set([x for x in output_stations if full_output_station_list.count(x) == 0])) )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0hVyNufqhc0"
      },
      "source": [
        "# Import helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK7YMXhnNMXa"
      },
      "source": [
        "import sys\n",
        "!export PYTHONPATH=\"\"\n",
        "\n",
        "sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir))\n",
        "\n",
        "from ann_helper import read_csv,normalize_in,process_data,process_data_vary_pred,initnw,conv_filter_generator\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "# from sklearn.utils import shuffle\n",
        "from scipy import stats\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "#   print('xxxxxxxxxxxxxx Using CPU xxxxxxxxxxxxxx')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VQKSR19qeH8"
      },
      "source": [
        "# Read and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl_fOymBNIrJ"
      },
      "source": [
        "single_daily = 8\n",
        "window_num = 10\n",
        "window_width = 11\n",
        "input_shape = (1,(single_daily+window_num)*len(input_var))\n",
        "output_shape = len(output_stations)\n",
        "\n",
        "nn_shape = [single_daily+window_num*window_width,]+hidden_layer_nodes+[output_shape,]\n",
        "\n",
        "\n",
        "\n",
        "# label and sort output stations by a given order\n",
        "locs = {'Emmaton':0,'Jersey Point':1,'Collinsville':2,'Rock Slough':3,'Antioch':4,\n",
        "        'Mallard':5, 'LosVaqueros':6, 'Martinez':7, 'MiddleRiver':8, 'Vict Intake':9,\n",
        "        'CVP Intake':10, 'CCFB_OldR':11}\n",
        "abbrev_map = {'rock slough':'ORRSL','rockslough':'ORRSL',\n",
        "            'emmaton':'EMM','jersey point':'JP','jerseypoint':'JP',\n",
        "            'antioch':'antioch','collinsville':'CO',\n",
        "            'mallard':'Mallard','mallard island':'Mallard',\n",
        "            'los vaqueros':'LosVaqueros','losvaqueros':'LosVaqueros',\n",
        "            'martinez':'MTZ',\n",
        "            'middle river':'MidR_intake','middleriver':'MidR_intake',\n",
        "            'victoria cannal':'Victoria_intake','vict intake':'Victoria_intake',\n",
        "            'cvp intake':'CVP_intake','clfct forebay':'CCFB',\n",
        "            'clfct forebay intake':'ccfb_intake','x2':'X2'};\n",
        "\n",
        "output_stations = sorted(output_stations,key=lambda x: locs[x])\n",
        "output_stations = [abbrev_map[x] if x in abbrev_map.keys() else x for x in output_stations]\n",
        "\n",
        "pred = [lead_day_for_station[station] for station in output_stations]\n",
        "\n",
        "# set same weight for all stations\n",
        "class_weights = np.ones(12)\n",
        "# if 'Martinez' in output_stations:\n",
        "#   class_weights[output_stations.index('Martinez')] = 10\n",
        "\n",
        "\n",
        "input_var_map = {'SAC':'SAC 0', 'Exp':'Exports',\n",
        "                 'SJR':'SJR','DICU':'DICU','Vern':'Vern EC',\n",
        "                 'SF_Tide':'SF_Tide', 'DXC':'DXC'}\n",
        "input_var_order = {'SAC 0':0, 'Exports':1,\n",
        "                 'SJR':2,'DICU':3,'Vern EC':4,\n",
        "                 'SF_Tide':5, 'DXC':6}\n",
        "\n",
        "try:\n",
        "    input_var = [input_var_map[var] for var in input_var]\n",
        "except Exception as e:\n",
        "    print('Input variables can only be selected in: SAC, Exp, SJR, DICU, Vern, SF_Tide, DXC.')\n",
        "    print('But got %s.' % e)\n",
        "\n",
        "sorted(input_var,key=lambda x: input_var_order[x])\n",
        "\n",
        "\n",
        "# read data from csv files\n",
        "x_data,y_data=read_csv(input_data_path,output_data_path,input_var,output_stations)\n",
        "test_mode = True\n",
        "\n",
        "# normalize data to 0.1 ~ 0.9\n",
        "[x_data,x_slope,x_bias] = normalize_in(x_data)\n",
        "[y_data,y_slope,y_bias] = normalize_in(y_data)\n",
        "\n",
        "# prepare input and output data sample pairs\n",
        "# x_data,y_data=process_data(x_data,y_data,single_daily+window_num*window_width,0,0)\n",
        "x_data,y_data=process_data_vary_pred(x_data,y_data,single_daily+window_num*window_width,0,0,predict_list=pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCgG6f-kqbRS"
      },
      "source": [
        "# Determine hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsO6AOfrfuyV"
      },
      "source": [
        "# adam optimizer settings\n",
        "batch_size = 32\n",
        "\n",
        "train_loc = locs[output_stations[0]]\n",
        "ann_name = '_'.join([abbrev_map[x.lower()][:4] if x.lower() in abbrev_map.keys() else x[:4] for x in output_stations])\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "# split 80% data for training, 20% for testing\n",
        "x_train_ori, x_test_ori, y_train0, y_test0 = train_test_split(x_data,\n",
        "                                                              y_data,\n",
        "                                                              test_size=0.2,\n",
        "                                                              random_state = 0)\n",
        "\n",
        "if test_mode:\n",
        "    x_train_ori = x_train_ori[:100]\n",
        "    x_test_ori = x_test_ori[:100]\n",
        "    y_train0 = y_train0[:100]\n",
        "    y_test0 = y_test0[:100]\n",
        "    epochs = 5\n",
        "else:\n",
        "    epochs = 200\n",
        "\n",
        "train_err = []\n",
        "test_err = []\n",
        "train_shape = len(x_train_ori)\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 1e-3\n",
        "    if epoch/epochs > 0.9:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch/epochs > .8:\n",
        "        lr *= 1e-3\n",
        "    elif epoch/epochs > .6:\n",
        "        lr *= 1e-2\n",
        "    elif epoch/epochs > .4:\n",
        "        lr *= 1e-1\n",
        "    elif epoch/epochs < .05:\n",
        "        lr *= epoch/5\n",
        "    if epoch/epochs < .05:\n",
        "        print('Learning rate: ', lr)\n",
        "    elif epoch/epochs in [0.9, 0.8, 0.6, 0.4]:\n",
        "        print('Learning rate changed to: ', lr)\n",
        "    return lr\n",
        "\n",
        "\n",
        "# custom loss function\n",
        "def weighted_mse(class_weights):\n",
        "    def custom_mse(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred:  A tensor resulting from a sigmoid\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "        # print('y_pred:', tf.keras.backend.int_shape(y_pred))\n",
        "        # print('y_true:', K.int_shape(y_true))\n",
        "        # calculating mean squared error\n",
        "        mse = tf.keras.backend.mean(class_weights * tf.keras.backend.square(y_pred - y_true), axis=-1)\n",
        "        return mse\n",
        "    return custom_mse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDmw5yGSqv32"
      },
      "source": [
        "# Build and train ANN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGftGCWXqZ6q"
      },
      "source": [
        "# Build model\n",
        "conv_filter_init = tf.constant_initializer(conv_filter_generator(single_days=single_daily,window_num=window_num,window_size = window_width))\n",
        "\n",
        "inputs = tf.keras.Input(shape=(len(input_var),single_daily+window_num*window_width))\n",
        "x = tf.keras.layers.Conv1D(single_daily+window_num,1, activation='relu',\n",
        "                           kernel_initializer=conv_filter_init,\n",
        "                           kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0, l2=0))(inputs)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "\n",
        "for nodes in hidden_layer_nodes:\n",
        "    x = tf.keras.layers.Dense(nodes, activation='sigmoid')(x)\n",
        "x = tf.keras.layers.Dense(output_shape)(x)\n",
        "outputs = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=weighted_mse(class_weights),\n",
        "    metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# define callbacks\n",
        "checkpoint_filepath = os.path.join('/content/drive','My Drive',google_drive_dir,\"models/Conv_MTL/%s/model.ckpt\"%ann_name)\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=0),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                       save_weights_only=True,\n",
        "                                       monitor='val_loss',mode='min',save_best_only=True)\n",
        "    ]\n",
        "\n",
        "start = time.time()\n",
        "history_callback=model.fit(x_train_ori,y_train0,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test_ori,y_test0),\n",
        "          callbacks=callbacks,verbose=2)\n",
        "end = time.time()\n",
        "print('Training finished in %d seconds' % (end-start) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S42SqIHBssBM"
      },
      "source": [
        "# Print results in terms of mean squared error (MSE) and mean absolute percentage error (MAPE) on training and test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ctd2V5Mercc"
      },
      "source": [
        "print(\"With trainable conv layer (initiated with pre-determined weights):\\n\")\n",
        "\n",
        "y_train_pred = np.clip(model.predict(x_train_ori),0,1)\n",
        "y_test_pred = np.clip(model.predict(x_test_ori),0,1)\n",
        "\n",
        "cell_width = 15\n",
        "header_format = \"|{:^%d}|{:^%d}|{:^%d}|{:^%d}|{:^%d}|\" % (cell_width, cell_width, cell_width, cell_width, cell_width)\n",
        "row_format = \"|{:<%d}|{:<%d.1f}|{:<%d.2%%}|{:<%d.1f}|{:<%d.2%%}|\" % (cell_width, cell_width, cell_width, cell_width, cell_width)\n",
        "\n",
        "for ii in range(output_shape):\n",
        "    if ii == 0:\n",
        "        print(\"-\"*(cell_width*5+6))\n",
        "        print(header_format.format('Station', 'Train MSE', 'Train MAPE', 'Test MSE', 'Test MAPE'))  # *row == row element wise\n",
        "        print(\"-\"*(cell_width*5+6))\n",
        "        print(\"-\"*(cell_width*5+6))\n",
        "\n",
        "    train_mse = np.mean(((y_train_pred[:,ii]-y_train0[:,ii])/y_slope[ii])**2)\n",
        "    train_mape = np.mean(abs(y_train_pred[:,ii]-y_train0[:,ii])/y_train0[:,ii])\n",
        "    test_mse = np.mean(((y_test_pred[:,ii]-y_test0[:,ii])/y_slope[ii])**2)\n",
        "    test_mape = np.mean(abs(y_test_pred[:,ii]-y_test0[:,ii])/y_test0[:,ii])\n",
        "\n",
        "    print(row_format.format(output_stations[ii],train_mse, train_mape, test_mse, test_mape))\n",
        "    print(\"-\"*(cell_width*5+6))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}