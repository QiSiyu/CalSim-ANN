{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_single_output_ANN_from_excel.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbLsjPUk3oIb",
        "colab_type": "text"
      },
      "source": [
        "## Training a Single-Output ANN from scratch\n",
        "\n",
        "Note: this script trains only one ANN for the station selected by user. Please set 'output_stations' to different stations for training multiple ANNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHPnGTMf95oI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f1e1e1c9-96a5-47bd-b317-6f8b39e0e871"
      },
      "source": [
        "######## User Settings ########\n",
        "\n",
        "#### 1. Select parameters to be used ####\n",
        "input_var = ['SAC','Exp','SJR','DICU','Vern','SF_Tide','DXC']\n",
        "#### 2. Select stations to be predicted ####\n",
        "# choose ONE of 'Emmaton','Jersey Point','Collinsville','Rock Slough'\n",
        "output_stations=['Rock Slough']\n",
        "#### 3. Specify directory to excel dataset and the helper script (folder name only) ####\n",
        "google_drive_dir = 'matlab_ANN'\n",
        "\n",
        "###### User Settings Finished ######\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "# Mount Google drive\n",
        "data_in_google_drive = True\n",
        "\n",
        "if data_in_google_drive:\n",
        "    drive.mount('/content/drive',force_remount=True)\n",
        "    xl_path = os.path.join('/content/drive','My Drive',google_drive_dir,\"ANN_data.xlsx\")\n",
        "    %tensorflow_version 1.x\n",
        "else:\n",
        "    xl_path = \"ANN_data.xlsx\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6Wd7z3l996j",
        "colab_type": "text"
      },
      "source": [
        "##1. Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6GxpZSUj0dF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import helper functions\n",
        "import sys\n",
        "!export PYTHONPATH=\"\"\n",
        "sys.path.append(os.path.join('/content/drive','My Drive',google_drive_dir))\n",
        "from ann_helper import read_data,normalize_in,writeF90,initnw,show_eval"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6348s2lzcfC",
        "colab_type": "code",
        "outputId": "9a86b866-366a-49fd-da5d-a80896147bb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "# from sklearn.utils import shuffle\n",
        "from scipy import stats\n",
        "\n",
        "test_mode = False\n",
        "\n",
        "locs = {'Emmaton':0,'Jersey Point':1,'Collinsville':2,'Rock Slough':3}\n",
        "abbrev_map = {'rock slough':'ORRSL','rockslough':'ORRSL',\n",
        "            'emmaton':'EMM','jersey point':'JP','jerseypoint':'JP',\n",
        "            'antioch':'antioch','collinsville':'CO',\n",
        "            'mallard':'Mallard','mallard island':'Mallard',\n",
        "            'los vaqueros':'LosVaqueros','losvaqueros':'LosVaqueros',\n",
        "            'martinez':'MTZ',\n",
        "            'middle river':'MidR_intake','MiddleRiver':'MidR_intake',\n",
        "            'victoria cannal':'Victoria_intake','Vict Intake':'Victoria_intake',\n",
        "            'cvp intake':'CVP_intake','clfct forebay':'CCFB',\n",
        "            'clfct forebay intake':'CCFB_intake','x2':'X2'};\n",
        "\n",
        "output_stations = sorted(output_stations,key=lambda x: locs[x])\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "#   print('xxxxxxxxxxxxxx Using CPU xxxxxxxxxxxxxx')\n",
        "else:\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "\n",
        "input_shape = (1,17*len(input_var))\n",
        "output_shape = 1\n",
        "nn_shape = [17*len(input_var),8,2,1] # number of neurons in each layer\n",
        "on_server = True\n",
        "\n",
        "# LM optimizer settings\n",
        "max_fail = 3\n",
        "epochs = 10\n",
        "init_mu = .05\n",
        "mu_max = 1e10\n",
        "target_mse = 0.\n",
        "\n",
        "# adam optimizer settings\n",
        "adam_epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "\n",
        "if test_mode or device_name != '/device:GPU:0':\n",
        "    epochs = 5\n",
        "else:\n",
        "    epochs = 100\n",
        "\n",
        "train_loc = locs[output_stations[0]]\n",
        "ann_name = abbrev_map[output_stations[0].lower()]\n",
        "start = time.time()\n",
        "\n",
        "# read data from excel\n",
        "x_data,y_data = read_data(xl_path,input_var,output_stations)\n",
        "end = time.time()\n",
        "print(\"loading data in %.2f seconds\" % (end-start) )\n",
        "    \n",
        "# normalize data to 0.1 ~ 0.9\n",
        "[x_norm,x_slope,x_bias] = normalize_in(x_data)\n",
        "[y_norm,y_slope,y_bias] = normalize_in(y_data)\n",
        "\n",
        "# split 80% for training, 20% for testing\n",
        "x_train_ori, x_test_ori, y_train0, y_test0 = train_test_split(x_norm,\n",
        "                                                              y_norm,\n",
        "                                                              test_size=0.2,\n",
        "                                                              random_state = 0)\n",
        "\n",
        "if test_mode:\n",
        "    x_train_ori = x_train_ori[:100]\n",
        "    x_test_ori = x_test_ori[:100]\n",
        "    y_train0 = y_train0[:100]\n",
        "    y_test0 = y_test0[:100]\n",
        "train_err = []\n",
        "test_err = []\n",
        "train_shape = len(x_train_ori)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Disgarding last 1 row(s) of data in output set...\n",
            "loading data in 27.14 seconds\n",
            "loading data in 27.17 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWm8BrKeZqU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ann_helper import Filter\n",
        "Filter??"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZNeDaHG0fyp",
        "colab_type": "code",
        "outputId": "7c84ecac-6345-444f-db95-7dbdbb573b9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# build artificial neural network\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.set_random_seed(1)\n",
        "\n",
        "x = tf.compat.v1.placeholder(tf.float32, [None, 17*len(input_var)], name='InputData')\n",
        "y = tf.compat.v1.placeholder(tf.float32, [None, 1], name='LabelData')\n",
        "\n",
        "init_val = initnw(list(zip(*(nn_shape[i:] for i in range(2)))),x_train_ori)\n",
        "\n",
        "W1 = tf.Variable(initial_value=init_val[0][0], name='w1',dtype='float32')\n",
        "b1 = tf.Variable(initial_value=init_val[0][1], name='b1',dtype='float32')\n",
        "W2 = tf.Variable(initial_value=init_val[1][0], name='w2',dtype='float32')\n",
        "b2 = tf.Variable(initial_value=init_val[1][1], name='b2',dtype='float32')\n",
        "W3 = tf.Variable(initial_value=init_val[2][0], name='w3',dtype='float32')\n",
        "b3 = tf.Variable(initial_value=init_val[2][1], name='b3',dtype='float32')\n",
        "\n",
        "with tf.compat.v1.name_scope('layer1'):\n",
        "    first_out = tf.sigmoid(tf.add(tf.matmul(x,W1),b1))\n",
        "with tf.compat.v1.name_scope('layer2'):\n",
        "    second_out = tf.sigmoid(tf.add(tf.matmul(first_out,W2),b2))\n",
        "# with tf.name_scope('layer3'):\n",
        "#     third_out = tf.sigmoid(tf.add(tf.matmul(second_out,W3),b3))\n",
        "with tf.compat.v1.name_scope('layer3'):\n",
        "    pred = tf.matmul(second_out, W3) + b3\n",
        "\n",
        "# residuals\n",
        "r = tf.subtract(pred,y)\n",
        "# loss function must be mse or sse for LM\n",
        "cost = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "\n",
        "# For adam optimizer:\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "initial_lr = 1e-2\n",
        "learning_rate = tf.compat.v1.train.exponential_decay(initial_lr, global_step,\n",
        "                                       40, 0.999, staircase=True)\n",
        "# Adam optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    adam_opt = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step = global_step)\n",
        "\n",
        "# LM optimizer\n",
        "with tf.compat.v1.name_scope('train_op'):\n",
        "    opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=1)\n",
        "\n",
        "with tf.compat.v1.name_scope('Accuracy'):\n",
        "    # Accuracy\n",
        "    acc = tf.reduce_mean(input_tensor=tf.square(tf.subtract(pred,y)))\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "\n",
        "# LM algorithm\n",
        "def jacobian(y, x):\n",
        "    loop_vars = [\n",
        "        tf.constant(0, tf.int32),\n",
        "        tf.TensorArray(tf.float32, size=train_shape),\n",
        "    ]\n",
        "\n",
        "    _, jacobian = tf.while_loop(\n",
        "        cond=lambda i, _: i < train_shape,\n",
        "        body=lambda i, res: (i+1, res.write(i, tf.reshape(tf.gradients(ys=y[i], xs=x), (-1,)))),\n",
        "        loop_vars=loop_vars)\n",
        "    return jacobian.stack()\n",
        "\n",
        "\n",
        "r_flat = tf.expand_dims(tf.reshape(r,[-1]),1)\n",
        "parms = [W1, b1, W2, b2, W3, b3]\n",
        "parms_sizes = [tf.size(input=p) for p in parms]\n",
        "j = tf.concat([jacobian(r_flat, p) for p in parms], 1)\n",
        "jT = tf.transpose(a=j)\n",
        "hess_approx = tf.matmul(jT, j)\n",
        "grad_approx = tf.matmul(jT, r_flat)\n",
        "\n",
        "mu = tf.compat.v1.placeholder(tf.float32, shape=[])\n",
        "\n",
        "store = [tf.Variable(tf.zeros(p.shape, dtype=tf.float32)) for p in parms]\n",
        "save_parms = [tf.compat.v1.assign(s, p) for s, p in zip(store, parms)]\n",
        "restore_parms = [tf.compat.v1.assign(p, s) for s, p in zip(store, parms)]\n",
        "\n",
        "wb_flat = tf.concat([tf.reshape(p,[-1,1]) for p in parms],axis=0)\n",
        "n = tf.add_n(parms_sizes)\n",
        "I = tf.eye(n, dtype=tf.float32)\n",
        "w_2 = tf.reduce_sum(input_tensor=tf.square(wb_flat))\n",
        "\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(init)\n",
        "print(\"Number of trainable parameters: %d\" % sess.run(n))\n",
        "\n",
        "\n",
        "#  lm algorithm\n",
        "dp_flat = tf.matmul(tf.linalg.inv(hess_approx + tf.multiply(mu, I)), grad_approx)\n",
        "dps = tf.split(dp_flat, parms_sizes, 0)\n",
        "\n",
        "for i in range(len(dps)):\n",
        "    dps[i] = tf.reshape(dps[i], parms[i].shape)\n",
        "lm = opt.apply_gradients(zip(dps, parms))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable parameters: 981\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVgyRTmYsLR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.compat.v1.train.Saver({'W1':W1,'b1':b1,'W2':W2,'b2':b2,'W3':W3,'b3':b3})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dy1XKPUeKd10",
        "colab_type": "code",
        "outputId": "69dc3b74-d7bc-466e-ef6a-ba1cd66777bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Adam optimizer training\n",
        "\n",
        "\n",
        "# Feed batch data\n",
        "def get_batch(inputX, inputY, batch_size):\n",
        "    duration = len(inputX)\n",
        "    for i in range(0,duration//batch_size):\n",
        "        idx = i*batch_size\n",
        "        yield inputX[idx:idx+batch_size], inputY[idx:idx+batch_size]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Start training\n",
        "\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        cost_list = []\n",
        "        lr_list = []\n",
        "        # Run the initializer\n",
        "        sess.run(init)\n",
        "\n",
        "        # Training cycle\n",
        "        total_batch = train_shape//batch_size\n",
        "        for epoch in range(adam_epochs):\n",
        "            avg_cost = 0.\n",
        "            ii = 0\n",
        "            # Loop over all batches\n",
        "            for batch_xs, batch_ys in get_batch(x_train_ori, y_train0,batch_size):\n",
        "                ii = ii +1\n",
        "                # Run optimization op (backprop), cost op (to get loss value)\n",
        "                # and summary nodes\n",
        "                _, c = sess.run([adam_opt, cost],\n",
        "                                 feed_dict={x: batch_xs, y: batch_ys})\n",
        "                # Write logs at every iteration\n",
        "                # Compute average loss\n",
        "                avg_cost += c / total_batch\n",
        "            lr = learning_rate.eval()\n",
        "            cost_list.append(avg_cost)\n",
        "            lr_list.append(lr)\n",
        "            # Display logs per epoch step\n",
        "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost),\n",
        "                  \"lr = \",\"{:.10f}\".format(lr))\n",
        "\n",
        "    print(\"Optimization Finished!\")\n",
        "\n",
        "    # Test model\n",
        "    # Calculate accuracy\n",
        "    print(\"Train Error:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "    print(\"Test Error:\", acc.eval({x: x_test_ori, y: y_test0}))\n",
        "    y_test_ = sess.run(pred,feed_dict={x:x_test_ori})\n",
        "    print(\"Test MSE:\", np.mean(np.square((y_test_-y_test0)/y_slope)))\n",
        "    print(\"Test MAPE:\", np.mean(np.abs(y_test_-y_test0)/y_test0))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost= 0.153603102 lr =  0.0098313550\n",
            "Epoch: 0002 cost= 0.004315675 lr =  0.0096655544\n",
            "Epoch: 0003 cost= 0.003086547 lr =  0.0095025497\n",
            "Epoch: 0004 cost= 0.002714624 lr =  0.0093422951\n",
            "Epoch: 0005 cost= 0.002485908 lr =  0.0091847414\n",
            "Epoch: 0006 cost= 0.002314884 lr =  0.0090298457\n",
            "Epoch: 0007 cost= 0.002173376 lr =  0.0088775624\n",
            "Epoch: 0008 cost= 0.002069232 lr =  0.0087278476\n",
            "Epoch: 0009 cost= 0.001987252 lr =  0.0085806567\n",
            "Epoch: 0010 cost= 0.001922094 lr =  0.0084275128\n",
            "Epoch: 0011 cost= 0.001857932 lr =  0.0082853874\n",
            "Epoch: 0012 cost= 0.001799109 lr =  0.0081456583\n",
            "Epoch: 0013 cost= 0.001755711 lr =  0.0080082864\n",
            "Epoch: 0014 cost= 0.001693196 lr =  0.0078732315\n",
            "Epoch: 0015 cost= 0.001631145 lr =  0.0077404534\n",
            "Epoch: 0016 cost= 0.001578224 lr =  0.0076099150\n",
            "Epoch: 0017 cost= 0.001535840 lr =  0.0074815778\n",
            "Epoch: 0018 cost= 0.001491965 lr =  0.0073554050\n",
            "Epoch: 0019 cost= 0.001460381 lr =  0.0072313598\n",
            "Epoch: 0020 cost= 0.001427996 lr =  0.0071022981\n",
            "Epoch: 0021 cost= 0.001398870 lr =  0.0069825212\n",
            "Epoch: 0022 cost= 0.001371124 lr =  0.0068647647\n",
            "Epoch: 0023 cost= 0.001345899 lr =  0.0067489943\n",
            "Epoch: 0024 cost= 0.001322616 lr =  0.0066351760\n",
            "Epoch: 0025 cost= 0.001299862 lr =  0.0065232776\n",
            "Epoch: 0026 cost= 0.001277912 lr =  0.0064132661\n",
            "Epoch: 0027 cost= 0.001253515 lr =  0.0063051092\n",
            "Epoch: 0028 cost= 0.001227533 lr =  0.0061987774\n",
            "Epoch: 0029 cost= 0.001202233 lr =  0.0060942383\n",
            "Epoch: 0030 cost= 0.001179420 lr =  0.0059854710\n",
            "Epoch: 0031 cost= 0.001158149 lr =  0.0058845291\n",
            "Epoch: 0032 cost= 0.001138323 lr =  0.0057852892\n",
            "Epoch: 0033 cost= 0.001119237 lr =  0.0056877243\n",
            "Epoch: 0034 cost= 0.001099341 lr =  0.0055918032\n",
            "Epoch: 0035 cost= 0.001080095 lr =  0.0054975008\n",
            "Epoch: 0036 cost= 0.001061762 lr =  0.0054047885\n",
            "Epoch: 0037 cost= 0.001044440 lr =  0.0053136395\n",
            "Epoch: 0038 cost= 0.001027805 lr =  0.0052240281\n",
            "Epoch: 0039 cost= 0.001012069 lr =  0.0051359269\n",
            "Epoch: 0040 cost= 0.000997225 lr =  0.0050442643\n",
            "Epoch: 0041 cost= 0.000982577 lr =  0.0049591945\n",
            "Epoch: 0042 cost= 0.000968508 lr =  0.0048755603\n",
            "Epoch: 0043 cost= 0.000954885 lr =  0.0047933366\n",
            "Epoch: 0044 cost= 0.000941779 lr =  0.0047124997\n",
            "Epoch: 0045 cost= 0.000929152 lr =  0.0046330262\n",
            "Epoch: 0046 cost= 0.000917282 lr =  0.0045548924\n",
            "Epoch: 0047 cost= 0.000905130 lr =  0.0044780769\n",
            "Epoch: 0048 cost= 0.000893662 lr =  0.0044025565\n",
            "Epoch: 0049 cost= 0.000880868 lr =  0.0043283100\n",
            "Epoch: 0050 cost= 0.000861830 lr =  0.0042510596\n",
            "Epoch: 0051 cost= 0.000846576 lr =  0.0041793678\n",
            "Epoch: 0052 cost= 0.000831085 lr =  0.0041088853\n",
            "Epoch: 0053 cost= 0.000816897 lr =  0.0040395907\n",
            "Epoch: 0054 cost= 0.000804432 lr =  0.0039714654\n",
            "Epoch: 0055 cost= 0.000792984 lr =  0.0039044886\n",
            "Epoch: 0056 cost= 0.000782399 lr =  0.0038386418\n",
            "Epoch: 0057 cost= 0.000772714 lr =  0.0037739053\n",
            "Epoch: 0058 cost= 0.000763836 lr =  0.0037102599\n",
            "Epoch: 0059 cost= 0.000755659 lr =  0.0036476888\n",
            "Epoch: 0060 cost= 0.000748087 lr =  0.0035825865\n",
            "Epoch: 0061 cost= 0.000740951 lr =  0.0035221679\n",
            "Epoch: 0062 cost= 0.000734124 lr =  0.0034627684\n",
            "Epoch: 0063 cost= 0.000727470 lr =  0.0034043705\n",
            "Epoch: 0064 cost= 0.000720967 lr =  0.0033469577\n",
            "Epoch: 0065 cost= 0.000714617 lr =  0.0032905131\n",
            "Epoch: 0066 cost= 0.000708407 lr =  0.0032350207\n",
            "Epoch: 0067 cost= 0.000702382 lr =  0.0031804633\n",
            "Epoch: 0068 cost= 0.000696449 lr =  0.0031268266\n",
            "Epoch: 0069 cost= 0.000690675 lr =  0.0030740942\n",
            "Epoch: 0070 cost= 0.000685218 lr =  0.0030192290\n",
            "Epoch: 0071 cost= 0.000679934 lr =  0.0029683115\n",
            "Epoch: 0072 cost= 0.000674820 lr =  0.0029182525\n",
            "Epoch: 0073 cost= 0.000669851 lr =  0.0028690377\n",
            "Epoch: 0074 cost= 0.000665033 lr =  0.0028206529\n",
            "Epoch: 0075 cost= 0.000660360 lr =  0.0027730847\n",
            "Epoch: 0076 cost= 0.000655796 lr =  0.0027263178\n",
            "Epoch: 0077 cost= 0.000651376 lr =  0.0026803399\n",
            "Epoch: 0078 cost= 0.000647058 lr =  0.0026351374\n",
            "Epoch: 0079 cost= 0.000642821 lr =  0.0025906970\n",
            "Epoch: 0080 cost= 0.000638703 lr =  0.0025444597\n",
            "Epoch: 0081 cost= 0.000634662 lr =  0.0025015485\n",
            "Epoch: 0082 cost= 0.000630720 lr =  0.0024593612\n",
            "Epoch: 0083 cost= 0.000626857 lr =  0.0024178852\n",
            "Epoch: 0084 cost= 0.000623075 lr =  0.0023771089\n",
            "Epoch: 0085 cost= 0.000619385 lr =  0.0023370204\n",
            "Epoch: 0086 cost= 0.000615756 lr =  0.0022976077\n",
            "Epoch: 0087 cost= 0.000612216 lr =  0.0022588600\n",
            "Epoch: 0088 cost= 0.000608751 lr =  0.0022207657\n",
            "Epoch: 0089 cost= 0.000605342 lr =  0.0021833135\n",
            "Epoch: 0090 cost= 0.000602025 lr =  0.0021443467\n",
            "Epoch: 0091 cost= 0.000598763 lr =  0.0021081835\n",
            "Epoch: 0092 cost= 0.000595574 lr =  0.0020726300\n",
            "Epoch: 0093 cost= 0.000592447 lr =  0.0020376761\n",
            "Epoch: 0094 cost= 0.000589379 lr =  0.0020033119\n",
            "Epoch: 0095 cost= 0.000586384 lr =  0.0019695272\n",
            "Epoch: 0096 cost= 0.000583437 lr =  0.0019363119\n",
            "Epoch: 0097 cost= 0.000580549 lr =  0.0019036572\n",
            "Epoch: 0098 cost= 0.000577722 lr =  0.0018715530\n",
            "Epoch: 0099 cost= 0.000574941 lr =  0.0018399904\n",
            "Epoch: 0100 cost= 0.000572228 lr =  0.0018071508\n",
            "Optimization Finished!\n",
            "Train Error: 0.00055365777\n",
            "Test Error: 0.00062346255\n",
            "Test MSE: 3388.844\n",
            "Test MAPE: 0.084234476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw49GOpORTqm",
        "colab_type": "code",
        "outputId": "19dfc277-e139-45e8-fb72-99d88f83b6ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        print(\"Train Error before LM:\", acc.eval({x: x_train_ori, y: y_train0}))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Error before LM: 0.00055365777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FrIJ-IAfF9A",
        "colab_type": "code",
        "outputId": "e88104e4-3ed6-4aa1-f2db-1225fec44f1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "feed_dict = {x: x_train_ori,\n",
        "             y: y_train0}\n",
        "feed_dict[mu] = init_mu\n",
        "\n",
        "# construct so-called feed dictionary to map placeholders to actual values\n",
        "\n",
        "validation_feed_dict = {x: x_test_ori,\n",
        "                        y: y_test0}\n",
        "\n",
        "train_break = False\n",
        "\n",
        "current_loss = sess.run(cost,{x: x_train_ori,y: y_train0})\n",
        "\n",
        "\n",
        "# Start training\n",
        "start = time.time()\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        epoch = 1\n",
        "        fail_step = 0\n",
        "        # Training cycle\n",
        "        while epoch < epochs and current_loss > target_mse:\n",
        "            if not(epoch%2) or epoch == 1:\n",
        "                val_loss = sess.run(cost, validation_feed_dict)\n",
        "                print('epoch: %3d , mu: %.5e, train loss: %.10f, val loss: %.10f'%(epoch,feed_dict[mu],current_loss,val_loss))\n",
        "            sess.run(save_parms)\n",
        "            while True:\n",
        "                start_step = time.time()\n",
        "                sess.run(lm, feed_dict)\n",
        "                new_loss = sess.run(cost,feed_dict)\n",
        "                # print('One update ended in %d seconds' % (time.time()-start_step))\n",
        "                if new_loss > current_loss:\n",
        "                    fail_step += 1\n",
        "                    feed_dict[mu] *= 10\n",
        "                    if feed_dict[mu] > mu_max or fail_step > max_fail:\n",
        "                        train_break = True\n",
        "                        break\n",
        "                    sess.run(restore_parms)\n",
        "                else:\n",
        "                    print(\"mu: %.5e, new loss: %.5f, previous loss: %.5f\"%(feed_dict[mu],new_loss,current_loss))\n",
        "                    fail_step = 0\n",
        "                    feed_dict[mu] /= 10\n",
        "                    feed_dict[mu] = max(1e-20,feed_dict[mu])\n",
        "                    current_loss = new_loss\n",
        "                    break\n",
        "            if train_break:\n",
        "                print('Failed for %d step(s), current mu = %.5e, stop training' % (fail_step,feed_dict[mu]))\n",
        "                break\n",
        "            epoch += 1\n",
        "    print(\"Location #%d Optimization Finished in %d seconds!\" %(train_loc, time.time() - start))\n",
        "\n",
        "    # Test model\n",
        "\n",
        "    y_train_predicted = sess.run(pred,feed_dict)\n",
        "    y_test_predicted = sess.run(pred,validation_feed_dict)\n",
        "\n",
        "    show_eval(y_train_predicted[:,0],\n",
        "              y_train0[:,0],\n",
        "              y_test_predicted[:,0],\n",
        "              y_test0[:,0],\n",
        "              y_slope,y_bias,ann_name)\n",
        "    writeF90('fnet_'+abbrev_map[output_stations[0].lower()]+\".f90\",\n",
        "             y_slope[0],y_bias[0],\n",
        "             sess.run(W1).transpose(),sess.run(b1),\n",
        "             sess.run(W2).transpose(),sess.run(b2),\n",
        "             sess.run(W3).transpose(),sess.run(b3))\n",
        "\n",
        "model_path = \"./%s/model.ckpt\" % (abbrev_map[output_stations[0].lower()])\n",
        "save_path = saver.save(sess, model_path)\n",
        "print(\"Model saved in path: %s\" % save_path)\n",
        "with sess.as_default():\n",
        "    with sess.graph.as_default():\n",
        "        saved_test_loss = acc.eval(validation_feed_dict)\n",
        "        print(\"Train Error after LM:\", acc.eval({x: x_train_ori, y: y_train0}))\n",
        "        print(\"Test Error after LM:\", saved_test_loss)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:   1 , mu: 5.00000e-02, train loss: 0.0005536578, val loss: 0.0006234625\n",
            "mu: 5.00000e-01, new loss: 0.00050, previous loss: 0.00055\n",
            "epoch:   2 , mu: 5.00000e-02, train loss: 0.0005037716, val loss: 0.0005768043\n",
            "mu: 5.00000e-01, new loss: 0.00048, previous loss: 0.00050\n",
            "mu: 5.00000e-01, new loss: 0.00048, previous loss: 0.00048\n",
            "epoch:   4 , mu: 5.00000e-02, train loss: 0.0004757314, val loss: 0.0005443083\n",
            "mu: 5.00000e-01, new loss: 0.00047, previous loss: 0.00048\n",
            "mu: 5.00000e-01, new loss: 0.00046, previous loss: 0.00047\n",
            "epoch:   6 , mu: 5.00000e-02, train loss: 0.0004642607, val loss: 0.0005313655\n",
            "mu: 5.00000e-01, new loss: 0.00046, previous loss: 0.00046\n",
            "mu: 5.00000e-02, new loss: 0.00045, previous loss: 0.00046\n",
            "epoch:   8 , mu: 5.00000e-03, train loss: 0.0004509130, val loss: 0.0005159411\n",
            "mu: 5.00000e-02, new loss: 0.00041, previous loss: 0.00045\n",
            "mu: 5.00000e-03, new loss: 0.00039, previous loss: 0.00041\n",
            "epoch:  10 , mu: 5.00000e-04, train loss: 0.0003856137, val loss: 0.0004390609\n",
            "mu: 5.00000e-03, new loss: 0.00033, previous loss: 0.00039\n",
            "mu: 5.00000e-03, new loss: 0.00031, previous loss: 0.00033\n",
            "epoch:  12 , mu: 5.00000e-04, train loss: 0.0003069793, val loss: 0.0003618547\n",
            "mu: 5.00000e-03, new loss: 0.00029, previous loss: 0.00031\n",
            "mu: 5.00000e-03, new loss: 0.00026, previous loss: 0.00029\n",
            "epoch:  14 , mu: 5.00000e-04, train loss: 0.0002642234, val loss: 0.0003180440\n",
            "mu: 5.00000e-03, new loss: 0.00025, previous loss: 0.00026\n",
            "mu: 5.00000e-03, new loss: 0.00025, previous loss: 0.00025\n",
            "epoch:  16 , mu: 5.00000e-04, train loss: 0.0002472297, val loss: 0.0002984229\n",
            "mu: 5.00000e-03, new loss: 0.00024, previous loss: 0.00025\n",
            "mu: 5.00000e-03, new loss: 0.00024, previous loss: 0.00024\n",
            "epoch:  18 , mu: 5.00000e-04, train loss: 0.0002368629, val loss: 0.0002870929\n",
            "mu: 5.00000e-03, new loss: 0.00023, previous loss: 0.00024\n",
            "mu: 5.00000e-03, new loss: 0.00023, previous loss: 0.00023\n",
            "epoch:  20 , mu: 5.00000e-04, train loss: 0.0002259450, val loss: 0.0002765898\n",
            "mu: 5.00000e-03, new loss: 0.00022, previous loss: 0.00023\n",
            "mu: 5.00000e-03, new loss: 0.00022, previous loss: 0.00022\n",
            "epoch:  22 , mu: 5.00000e-04, train loss: 0.0002171965, val loss: 0.0002682940\n",
            "mu: 5.00000e-03, new loss: 0.00021, previous loss: 0.00022\n",
            "mu: 5.00000e-03, new loss: 0.00021, previous loss: 0.00021\n",
            "epoch:  24 , mu: 5.00000e-04, train loss: 0.0002102260, val loss: 0.0002619529\n",
            "mu: 5.00000e-03, new loss: 0.00021, previous loss: 0.00021\n",
            "mu: 5.00000e-03, new loss: 0.00020, previous loss: 0.00021\n",
            "epoch:  26 , mu: 5.00000e-04, train loss: 0.0002038003, val loss: 0.0002551689\n",
            "mu: 5.00000e-03, new loss: 0.00020, previous loss: 0.00020\n",
            "mu: 5.00000e-03, new loss: 0.00020, previous loss: 0.00020\n",
            "epoch:  28 , mu: 5.00000e-04, train loss: 0.0001987394, val loss: 0.0002495583\n",
            "mu: 5.00000e-03, new loss: 0.00020, previous loss: 0.00020\n",
            "mu: 5.00000e-03, new loss: 0.00019, previous loss: 0.00020\n",
            "epoch:  30 , mu: 5.00000e-04, train loss: 0.0001939582, val loss: 0.0002444542\n",
            "mu: 5.00000e-03, new loss: 0.00019, previous loss: 0.00019\n",
            "mu: 5.00000e-03, new loss: 0.00019, previous loss: 0.00019\n",
            "epoch:  32 , mu: 5.00000e-04, train loss: 0.0001901657, val loss: 0.0002402822\n",
            "mu: 5.00000e-03, new loss: 0.00019, previous loss: 0.00019\n",
            "mu: 5.00000e-03, new loss: 0.00019, previous loss: 0.00019\n",
            "epoch:  34 , mu: 5.00000e-04, train loss: 0.0001870718, val loss: 0.0002366777\n",
            "mu: 5.00000e-03, new loss: 0.00019, previous loss: 0.00019\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00019\n",
            "epoch:  36 , mu: 5.00000e-04, train loss: 0.0001847488, val loss: 0.0002337240\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  38 , mu: 5.00000e-04, train loss: 0.0001829312, val loss: 0.0002314315\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  40 , mu: 5.00000e-04, train loss: 0.0001814369, val loss: 0.0002293834\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  42 , mu: 5.00000e-04, train loss: 0.0001801524, val loss: 0.0002277619\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  44 , mu: 5.00000e-04, train loss: 0.0001790279, val loss: 0.0002263165\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  46 , mu: 5.00000e-04, train loss: 0.0001780034, val loss: 0.0002250702\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  48 , mu: 5.00000e-04, train loss: 0.0001770406, val loss: 0.0002240531\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  50 , mu: 5.00000e-04, train loss: 0.0001761218, val loss: 0.0002231042\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "mu: 5.00000e-03, new loss: 0.00018, previous loss: 0.00018\n",
            "epoch:  52 , mu: 5.00000e-04, train loss: 0.0001752498, val loss: 0.0002222442\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00018\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  54 , mu: 5.00000e-05, train loss: 0.0001738771, val loss: 0.0002213971\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  56 , mu: 5.00000e-04, train loss: 0.0001716123, val loss: 0.0002188467\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  58 , mu: 5.00000e-04, train loss: 0.0001707726, val loss: 0.0002182869\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  60 , mu: 5.00000e-04, train loss: 0.0001700619, val loss: 0.0002178192\n",
            "mu: 5.00000e-04, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  62 , mu: 5.00000e-04, train loss: 0.0001681569, val loss: 0.0002164888\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  64 , mu: 5.00000e-04, train loss: 0.0001676693, val loss: 0.0002161242\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  66 , mu: 5.00000e-04, train loss: 0.0001672333, val loss: 0.0002156774\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  68 , mu: 5.00000e-04, train loss: 0.0001667798, val loss: 0.0002153971\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  70 , mu: 5.00000e-04, train loss: 0.0001663019, val loss: 0.0002149966\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  72 , mu: 5.00000e-04, train loss: 0.0001658615, val loss: 0.0002145902\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  74 , mu: 5.00000e-04, train loss: 0.0001654764, val loss: 0.0002141289\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "epoch:  76 , mu: 5.00000e-04, train loss: 0.0001651570, val loss: 0.0002138037\n",
            "mu: 5.00000e-03, new loss: 0.00017, previous loss: 0.00017\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00017\n",
            "epoch:  78 , mu: 5.00000e-04, train loss: 0.0001648899, val loss: 0.0002134777\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-02, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  80 , mu: 5.00000e-03, train loss: 0.0001647582, val loss: 0.0002132846\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  82 , mu: 5.00000e-04, train loss: 0.0001645423, val loss: 0.0002130306\n",
            "mu: 5.00000e-02, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  84 , mu: 5.00000e-04, train loss: 0.0001644415, val loss: 0.0002128831\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  86 , mu: 5.00000e-04, train loss: 0.0001642570, val loss: 0.0002126322\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  88 , mu: 5.00000e-04, train loss: 0.0001640968, val loss: 0.0002125267\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  90 , mu: 5.00000e-04, train loss: 0.0001639504, val loss: 0.0002124161\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  92 , mu: 5.00000e-04, train loss: 0.0001638183, val loss: 0.0002122808\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  94 , mu: 5.00000e-04, train loss: 0.0001636924, val loss: 0.0002120701\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  96 , mu: 5.00000e-04, train loss: 0.0001635811, val loss: 0.0002119488\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "epoch:  98 , mu: 5.00000e-04, train loss: 0.0001634730, val loss: 0.0002118719\n",
            "mu: 5.00000e-03, new loss: 0.00016, previous loss: 0.00016\n",
            "mu: 5.00000e-04, new loss: 0.00016, previous loss: 0.00016\n",
            "Location #3 Optimization Finished in 2453 seconds!\n",
            "train mape:  0.0501\n",
            "train mse:  888.23\n",
            "test mape:  0.0530\n",
            "test mse: 1151.97\n",
            "<class 'numpy.float32'>\n",
            "Model saved in path: ./ORRSL/model.ckpt\n",
            "Train Error after LM: 0.000163413\n",
            "Test Error after LM: 0.00021193367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO3TvhKyz30q",
        "colab_type": "code",
        "outputId": "cddd0f6f-d577-45ef-b786-f222a54c5b02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "## download fortran file and trained model\n",
        "model_dir = abbrev_map[output_stations[0].lower()]\n",
        "from google.colab import files\n",
        "ann_zip = ann_name +'.zip'\n",
        "f90name = 'fnet_'+ann_name+'.f90'\n",
        "!zip -r /content/$ann_zip $model_dir\n",
        "!zip -r /content/$ann_zip $f90name"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "updating: ORRSL/ (stored 0%)\n",
            "updating: ORRSL/model.ckpt.data-00000-of-00001 (deflated 5%)\n",
            "updating: ORRSL/checkpoint (deflated 42%)\n",
            "updating: ORRSL/model.ckpt.meta (deflated 86%)\n",
            "updating: ORRSL/model.ckpt.index (deflated 32%)\n",
            "\tzip warning: name not matched: fnet_ORRSL.f90\n",
            "\n",
            "zip error: Nothing to do! (try: zip -r /content/ORRSL.zip . -i fnet_ORRSL.f90)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN1miFnUkHiE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "8698e13a-ab0c-46b7-d706-fb2034538465"
      },
      "source": [
        "files.download('/content/%s' % ann_zip)\n",
        "# files.download('%s.jpg'%(output_stations[0]+str(fig_index)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-3487b0050831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mann_zip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# files.download('%s.jpg'%(output_stations[0]+str(fig_index)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Failed to fetch"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UodHSRJMsGbB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2dade9c0-c413-4c73-b902-49c3861e0e0a"
      },
      "source": [
        "import matplotlib\n",
        "import scipy\n",
        "import pandas as pd\n",
        "print(np.__version__,scipy.__version__,pd.__version__,tf.__version__)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.18.4 1.4.1 1.0.3 1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}